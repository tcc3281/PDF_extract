from modules.tools import search_tool, extract_pdf, chunk_and_embed, check_chunks
from modules.states import AgentState
from langchain.prompts import ChatPromptTemplate
from typing import List, Dict, Any
import json
import logging
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from datetime import datetime
from pydantic import BaseModel, Field

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

load_dotenv()

def get_llm(api_key: str, model_name: str) -> ChatOpenAI:
    """T·∫°o ChatOpenAI instance"""
    return ChatOpenAI(
        model=model_name,
        temperature=0,
        api_key=api_key
    )

# Global LLM instance (will be set by the first call)
_global_llm = None

def set_global_llm(api_key: str, model_name: str):
    """Set global LLM instance"""
    global _global_llm
    _global_llm = ChatOpenAI(
        model=model_name,
        temperature=0,
        api_key=api_key
    )

def get_global_llm():
    """Get global LLM instance"""
    return _global_llm

def extracted_agent(state: AgentState) -> AgentState:
    retry_count = state.get("retry_count_a1", 0)
    logging.info(f"üöÄ Agent A1: B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t PDF (retry: {retry_count})...")
    
    if retry_count >= 3:
        logging.error("‚ùå Agent A1: ƒê√£ th·ª≠ 3 l·∫ßn nh∆∞ng kh√¥ng th√†nh c√¥ng")
        return {
            "error": "Invalid PDF after 3 retries", 
            "retry_count_a1": retry_count + 1,
            "messages": []
        }
    
    result = extract_pdf.invoke({"pdf_path": state["file_path"]})
    
    if result["error"]:
        logging.error(f"‚ùå Agent A1: L·ªói khi tr√≠ch xu·∫•t PDF - {result['error']}")
        return {
            "error": result["error"],
            "retry_count_a1": retry_count + 1,
            "messages": []
        }
    else:
        logging.info(f"‚úÖ Agent A1: ƒê√£ tr√≠ch xu·∫•t th√†nh c√¥ng {len(result['cleaned_text'])} k√Ω t·ª±")
        return {
            "cleaned_text": result["cleaned_text"],
            "error": None,
            "retry_count_a1": retry_count + 1,
            "messages": []
        }

def chunked_and_embedded_agent(state: AgentState) -> AgentState:
    """Agent ph√¢n ƒëo·∫°n v√† t·∫°o embeddings"""
    retry_count = state.get("retry_count_a2", 0)
    logging.info(f"üöÄ Agent A2: B·∫Øt ƒë·∫ßu chia nh·ªè v√† t·∫°o embeddings (retry: {retry_count})...")
    
    if retry_count >= 3:
        logging.error("‚ùå Agent A2: ƒê√£ th·ª≠ 3 l·∫ßn nh∆∞ng kh√¥ng th√†nh c√¥ng")
        return {
            "error": "Invalid chunks after 3 retries", 
            "retry_count_a2": retry_count + 1,
            "messages": []
        }
    
    if state["error"] or not state["cleaned_text"]:
        logging.error(f"‚ùå Agent A2: Kh√¥ng c√≥ text ƒë·ªÉ x·ª≠ l√Ω - {state['error'] or 'No cleaned text'}")
        return {
            "error": state["error"] or "No cleaned text", 
            "retry_count_a2": retry_count + 1,
            "messages": []
        }
    
    # L·∫•y chunk settings t·ª´ messages n·∫øu c√≥
    chunk_size, chunk_overlap = 2000, 200
    for msg in state.get("messages", []):
        if msg.get("to") == "agent_a2" and msg.get("action") == "adjust_chunk":
            chunk_size = int(msg.get("chunk_size", 2000))
            chunk_overlap = int(msg.get("chunk_overlap", 200))
            logging.info(f"üîÑ Agent A2: ƒêi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc chunk={chunk_size}, overlap={chunk_overlap}")
            break
    
    # T·∫°o file_id t·ª´ file_path
    file_id = Path(state["file_path"]).stem if state.get("file_path") else "unknown"
    
    result = chunk_and_embed.invoke({
        "text": state["cleaned_text"],
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "file_id": file_id,
        "api_key": state.api_key,
        "embedding_model": state.embedding_model
    })

    if result.get("error"):
        logging.error(f"‚ùå Agent A2: L·ªói khi t·∫°o embeddings - {result['error']}")
        return {
            "error": result["error"],
            "retry_count_a2": retry_count + 1,
            "messages": []
        }

    if not check_chunks(result["chunks"]):
        logging.warning("‚ö†Ô∏è Agent A2: Chunks kh√¥ng h·ª£p l·ªá, th·ª≠ ƒëi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc...")
        return {
            "error": "Invalid chunks",
            "retry_count_a2": retry_count + 1,
            "messages": [{
                "to": "agent_a2", 
                "action": "adjust_chunk", 
                "chunk_size": max(chunk_size - 500, 800), 
                "chunk_overlap": max(chunk_overlap - 50, 50)
            }]
        }

    logging.info(f"‚úÖ Agent A2: ƒê√£ t·∫°o {len(result['chunks'])} chunks v√† embeddings th√†nh c√¥ng")
    return {
        "chunks": result["chunks"],
        "embeddings": result["embeddings"],
        "db": result["db"],
        "retry_count_a2": retry_count + 1,
        "error": None,
        "messages": []  # Clear messages
    }

summarize_prompt = ChatPromptTemplate.from_messages([
    ("system", """Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr√≠ch xu·∫•t th√¥ng tin quan tr·ªçng, ch√≠nh x√°c v√† ng·∫Øn g·ªçn t·ª´ vƒÉn b·∫£n. 
T·∫≠p trung v√†o:
1. D·ªØ ki·ªán ch√≠nh (facts) v√† th√¥ng tin c·ªët l√µi
2. T√™n ng∆∞·ªùi, t·ªï ch·ª©c quan tr·ªçng
3. ƒê·ªãa ƒëi·ªÉm v√† th·ªùi gian c·ª• th·ªÉ
4. S·ªë li·ªáu ƒë·ªãnh l∆∞·ª£ng v√† th·ªëng k√™
5. M·ªëi quan h·ªá gi·ªØa c√°c th·ª±c th·ªÉ

B·ªè qua:
- Th√¥ng tin tr√πng l·∫∑p
- Chi ti·∫øt kh√¥ng quan tr·ªçng
- N·ªôi dung mang t√≠nh qu·∫£ng c√°o
- ƒê√°nh gi√° ch·ªß quan

T√≥m t·∫Øt ng·∫Øn g·ªçn, s√∫c t√≠ch (t·ªëi ƒëa 100 t·ª´), ch·ªâ gi·ªØ l·∫°i th√¥ng tin quan tr·ªçng nh·∫•t."""),
    ("user", "{text}")
])

extract_prompt = ChatPromptTemplate.from_messages([
    ("system", """Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ (entities) quan tr·ªçng t·ª´ vƒÉn b·∫£n.

QUAN TR·ªåNG: Lu√¥n tr·∫£ v·ªÅ JSON h·ª£p l·ªá theo ƒë·ªãnh d·∫°ng:
{
  "entities": {
    "names": ["t√™n ng∆∞·ªùi", "t√™n c√¥ng ty", "t√™n t·ªï ch·ª©c"],
    "dates": ["ng√†y th√°ng", "th·ªùi gian"],
    "locations": ["ƒë·ªãa ƒëi·ªÉm", "th√†nh ph·ªë", "qu·ªëc gia"],
    "numbers": ["s·ªë li·ªáu", "ph·∫ßn trƒÉm", "ti·ªÅn t·ªá"]
  }
}

V√≠ d·ª•:
Input: "C√¥ng ty ABC c√≥ 1000 nh√¢n vi√™n t·∫°i H√† N·ªôi t·ª´ nƒÉm 2020"
Output:
{
  "entities": {
    "names": ["C√¥ng ty ABC"],
    "dates": ["nƒÉm 2020"],
    "locations": ["H√† N·ªôi"],
    "numbers": ["1000 nh√¢n vi√™n"]
  }
}

N·∫øu kh√¥ng t√¨m th·∫•y entities n√†o, tr·∫£ v·ªÅ:
{
  "entities": {
    "names": [],
    "dates": [],
    "locations": [],
    "numbers": []
  }
}"""),
    ("user", "VƒÉn b·∫£n: {text}")
])

final_summarize_prompt = ChatPromptTemplate.from_messages([
    ("system", """T·ªïng h·ª£p c√°c ph·∫ßn th√¥ng tin ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t th√†nh m·ªôt b·∫£n t√≥m t·∫Øt ho√†n ch·ªânh, ng·∫Øn g·ªçn v√† c√≥ c·∫•u tr√∫c.
Y√™u c·∫ßu:
1. Ng·∫Øn g·ªçn, s√∫c t√≠ch, kh√¥ng qu√° 500 t·ª´
2. Ch·ªâ gi·ªØ l·∫°i th√¥ng tin quan tr·ªçng v√† c√≥ gi√° tr·ªã cao
3. S·∫Øp x·∫øp th√¥ng tin theo th·ª© t·ª± logic v√† d·ªÖ hi·ªÉu
4. Li√™n k·∫øt c√°c th√¥ng tin c√≥ li√™n quan v·ªõi nhau
5. ƒê·∫£m b·∫£o ch√≠nh x√°c v√† kh√°ch quan
6. T·∫≠p trung v√†o d·ªØ ki·ªán, s·ªë li·ªáu v√† m·ªëi quan h·ªá gi·ªØa c√°c th·ª±c th·ªÉ
7. Lo·∫°i b·ªè th√¥ng tin tr√πng l·∫∑p, kh√¥ng quan tr·ªçng ho·∫∑c mang t√≠nh ch·ªß quan

M·ª•c ƒë√≠ch: Gi√∫p ng∆∞·ªùi ƒë·ªçc n·∫Øm b·∫Øt nhanh ch√≥ng nh·ªØng th√¥ng tin quan tr·ªçng nh·∫•t t·ª´ vƒÉn b·∫£n g·ªëc."""),
    ("user", "{summaries}")
])

fallback_extract_prompt = ChatPromptTemplate.from_messages([
    ("system", """H√£y li·ªát k√™ t·ª´ng d√≤ng nh·ªØng th√¥ng tin quan tr·ªçng b·∫°n t√¨m th·∫•y:

T√™n ng∆∞·ªùi/c√¥ng ty:
- [li·ªát k√™ n·∫øu c√≥]

Ng√†y th√°ng:
- [li·ªát k√™ n·∫øu c√≥]

ƒê·ªãa ƒëi·ªÉm:
- [li·ªát k√™ n·∫øu c√≥]

S·ªë li·ªáu:
- [li·ªát k√™ n·∫øu c√≥]

N·∫øu kh√¥ng c√≥ th√¨ ghi "Kh√¥ng c√≥"."""),
    ("user", "VƒÉn b·∫£n: {text}")
])

def extract_entities_from_text(text_response: str) -> Dict:
    """Extract entities t·ª´ text response thay v√¨ JSON"""
    entities = {"names": [], "dates": [], "locations": [], "numbers": []}
    
    try:
        lines = text_response.split('\n')
        current_category = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # X√°c ƒë·ªãnh category
            if "t√™n ng∆∞·ªùi" in line.lower() or "c√¥ng ty" in line.lower():
                current_category = "names"
            elif "ng√†y" in line.lower() or "th√°ng" in line.lower():
                current_category = "dates"  
            elif "ƒë·ªãa ƒëi·ªÉm" in line.lower() or "v·ªã tr√≠" in line.lower():
                current_category = "locations"
            elif "s·ªë li·ªáu" in line.lower() or "s·ªë" in line.lower():
                current_category = "numbers"
            elif line.startswith('- ') and current_category:
                # Extract item
                item = line[2:].strip()
                if item and item.lower() != "kh√¥ng c√≥" and len(item) > 2:
                    entities[current_category].append(item)
    
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è L·ªói khi extract entities t·ª´ text: {str(e)}")
    
    return entities

def analyze_chunk_batch_with_mode(chunk: str, use_fallback: bool = False) -> Dict:
    """X·ª≠ l√Ω m·ªôt chunk ƒë∆°n l·∫ª v·ªõi t√πy ch·ªçn s·ª≠ d·ª•ng fallback prompt"""
    try:
        llm = get_global_llm()
        if not llm:
            raise ValueError("Global LLM not initialized")
            
        # X·ª≠ l√Ω summary
        try:
            summary_result = llm.invoke(summarize_prompt.format(text=chunk))
            summary = summary_result.content if hasattr(summary_result, "content") else str(summary_result)
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Summary generation failed: {str(e)}")
            summary = f"Summary processing error for chunk: {chunk[:100]}..."
        
        # X·ª≠ l√Ω entities v·ªõi 2 approaches kh√°c nhau
        if use_fallback:
            # Approach 1: Non-JSON fallback
            try:
                entities_result = llm.invoke(fallback_extract_prompt.format(text=chunk))
                entities_content = entities_result.content if hasattr(entities_result, "content") else str(entities_result)
                
                logging.info(f"üîÑ Fallback: S·ª≠ d·ª•ng text extraction thay v√¨ JSON")
                entities = extract_entities_from_text(entities_content)
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Fallback extraction failed: {str(e)}")
                entities = {"names": [], "dates": [], "locations": [], "numbers": []}
            
        else:
            # Approach 2: Standard JSON v·ªõi backup
            try:
                entities_result = llm.invoke(extract_prompt.format(text=chunk))
                entities_content = entities_result.content if hasattr(entities_result, "content") else str(entities_result)
                
                # Clean v√† validate JSON content tr∆∞·ªõc khi parse
                entities_content = entities_content.strip()
                
                # Remove markdown code blocks
                if entities_content.startswith("```json"):
                    entities_content = entities_content.replace("```json", "").replace("```", "").strip()
                elif entities_content.startswith("```"):
                    entities_content = entities_content.replace("```", "").strip()
                
                # Ki·ªÉm tra n·∫øu content qu√° ng·∫Øn ho·∫∑c kh√¥ng h·ª£p l·ªá
                if len(entities_content) < 10 or not entities_content.startswith("{"):
                    raise ValueError(f"Invalid JSON response: '{entities_content[:50]}...'")
                
                entities_data = json.loads(entities_content)
                
                # Extract entities t·ª´ response
                if "entities" in entities_data:
                    entities = entities_data["entities"]
                else:
                    entities = entities_data
                    
                # Validate structure
                if not isinstance(entities, dict):
                    raise ValueError("Invalid entities structure - not a dict")
                    
                # Ensure all required keys exist
                for key in ["names", "dates", "locations", "numbers"]:
                    if key not in entities:
                        entities[key] = []
                    elif not isinstance(entities[key], list):
                        entities[key] = []
                        
            except (json.JSONDecodeError, KeyError, ValueError) as e:
                logging.warning(f"‚ö†Ô∏è JSON parsing failed: {str(e)[:100]}. Content: '{entities_content[:100] if 'entities_content' in locals() else 'N/A'}'")
                # Backup: S·ª≠ d·ª•ng text extraction
                try:
                    entities = extract_entities_from_text(entities_content if 'entities_content' in locals() else "")
                except Exception as e2:
                    logging.warning(f"‚ö†Ô∏è Backup text extraction also failed: {str(e2)}")
                    entities = {"names": [], "dates": [], "locations": [], "numbers": []}
            except Exception as e:
                logging.error(f"‚ùå Unexpected error in JSON processing: {str(e)}")
                entities = {"names": [], "dates": [], "locations": [], "numbers": []}
        
        # Chu·∫©n h√≥a entities
        for key in entities:
            if isinstance(entities[key], list):
                entities[key] = [e for e in entities[key] if isinstance(e, str) and len(e.strip()) > 2]
            else:
                entities[key] = []
        
        result = {
            "summary": summary,
            "entities": entities,
            "text": chunk
        }
        
        entities_count = sum(len(v) for v in entities.values())
        prompt_type = "fallback" if use_fallback else "standard"
        logging.info(f"‚úÖ ({prompt_type}) Chunk processed: {len(chunk)} chars, entities: {entities_count} items")
        
        if entities_count > 0:
            logging.info(f"   üìã Entities found: {dict((k, len(v)) for k, v in entities.items())}")
        
        return result
        
    except Exception as e:
        logging.error(f"‚ùå Critical error processing chunk: {str(e)}")
        return {
            "summary": "Error processing this chunk",
            "entities": {"names": [], "dates": [], "locations": [], "numbers": []},
            "text": chunk
        }

def analyze_batch_parallel_with_mode(batch: List[str], max_workers: int = 10, use_fallback: bool = False) -> List[Dict]:
    """X·ª≠ l√Ω batch v·ªõi parallel processing v√† t√πy ch·ªçn fallback prompt"""
    try:
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_chunk = {
                executor.submit(analyze_chunk_batch_with_mode, chunk, use_fallback): chunk 
                for chunk in batch
            }
            
            for future in as_completed(future_to_chunk):
                try:
                    result = future.result(timeout=30)
                    if result:
                        results.append(result)
                except Exception as e:
                    logging.error(f"‚ùå Timeout or error in parallel processing: {str(e)}")
                    results.append({
                        "summary": "Error processing chunk",
                        "entities": {"names": [], "dates": [], "locations": [], "numbers": []},
                        "text": ""
                    })
        
        return results
    except Exception as e:
        logging.error(f"‚ùå Error in parallel batch processing: {str(e)}")
        return []

def count_tokens_estimate(text: str) -> int:
    """∆Ø·ªõc t√≠nh s·ªë tokens (kho·∫£ng 3.5 k√Ω t·ª± = 1 token cho ti·∫øng Vi·ªát)"""
    return len(text) // 3

def chunk_summaries_for_final(summaries: List[str], max_tokens: int = 150000) -> List[List[str]]:
    """Chia summaries th√†nh c√°c chunks l·ªõn h∆°n ƒë·ªÉ t·∫≠n d·ª•ng token limit cao"""
    chunks = []
    current_chunk = []
    current_tokens = 0
    
    for summary in summaries:
        summary_tokens = count_tokens_estimate(summary)
        
        if current_tokens + summary_tokens > max_tokens and current_chunk:
            chunks.append(current_chunk)
            current_chunk = [summary]
            current_tokens = summary_tokens
        else:
            current_chunk.append(summary)
            current_tokens += summary_tokens
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks

def analyzed_agent(state: AgentState) -> AgentState:
    """Agent ph√¢n t√≠ch n·ªôi dung"""
    if not state.api_key or not state.model_name:
        return {"error": "API key and model name are required", "messages": state.get("messages", [])}
        
    # Set global LLM
    set_global_llm(state.api_key, state.model_name)
    
    retry_count = int(state.get("retry_count_analyze", 0))
    use_fallback = retry_count > 0  # S·ª≠ d·ª•ng fallback prompt t·ª´ retry th·ª© 2
    
    logging.info(f"üöÄ Agent Analyze: B·∫Øt ƒë·∫ßu ph√¢n t√≠ch n·ªôi dung ({'fallback mode' if use_fallback else 'standard mode'})...")
    
    if retry_count >= 3:
        logging.error("‚ùå Agent Analyze: ƒê√£ th·ª≠ 3 l·∫ßn nh∆∞ng kh√¥ng th√†nh c√¥ng")
        return {"error": "Analysis failed after 3 retries", "messages": []}
    
    # Ki·ªÉm tra chunks availability - nh∆∞ng KH√îNG check error cho retry case
    if not state.get("chunks"):
        logging.error(f"‚ùå Agent Analyze: Kh√¥ng c√≥ chunks ƒë·ªÉ ph√¢n t√≠ch")
        return {"error": "No chunks available", "messages": []}
    
    # Check cho error CH·ªà KHI kh√¥ng ph·∫£i retry case
    if state.get("error") and retry_count == 0:
        error_msg = state.get("error")
        logging.error(f"‚ùå Agent Analyze: L·ªói t·ª´ agent tr∆∞·ªõc - {error_msg}")
        return {"error": error_msg, "messages": []}
    
    # Clear error state n·∫øu ƒëang retry
    if retry_count > 0:
        logging.info(f"üîÑ Agent Analyze: Clearing previous error for retry {retry_count}")
    
    try:
        batch_size = 20
        chunks = state["chunks"]
        batches = [chunks[i:i + batch_size] for i in range(0, len(chunks), batch_size)]
        logging.info(f"üìä Agent Analyze: X·ª≠ l√Ω {len(batches)} batches v·ªõi batch_size={batch_size} ({'fallback' if use_fallback else 'standard'} mode)...")

        summaries = []
        entities = {"names": [], "dates": [], "locations": [], "numbers": []}
        failed_chunks = []
        
        # X·ª≠ l√Ω c√°c batches v·ªõi prompt ph√π h·ª£p
        for i, batch in enumerate(batches):
            if i > 0:
                time.sleep(0.2)
            
            logging.info(f"üîÑ Processing batch {i+1}/{len(batches)} v·ªõi {len(batch)} chunks...")
            
            batch_results = analyze_batch_parallel_with_mode(batch, max_workers=min(15, len(batch)), use_fallback=use_fallback)
            
            if not batch_results:
                logging.warning(f"‚ö†Ô∏è Batch {i+1} kh√¥ng c√≥ k·∫øt qu·∫£")
                failed_chunks.extend(batch)
                continue
                
            for result in batch_results:
                if not result:
                    continue
                    
                if isinstance(result.get("summary"), str) and result["summary"].strip():
                    summaries.append(result["summary"])
                    
                if result.get("entities"):
                    for key in entities:
                        if key in result["entities"] and isinstance(result["entities"][key], list):
                            entities[key].extend(result["entities"][key])
                else:
                    failed_chunks.append(result.get("text", ""))
        
        # Ki·ªÉm tra k·∫øt qu·∫£ ph√¢n t√≠ch
        total_entities = sum(len(entities[key]) for key in entities)
        
        if total_entities == 0:
            logging.warning(f"‚ö†Ô∏è Agent Analyze: Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c entities n√†o (retry: {retry_count})")
            
            # Strategy 1: Th·ª≠ fallback prompt (retry 0 -> 1)
            if retry_count == 0:
                logging.info("üîÑ Strategy 1: Th·ª≠ l·∫°i v·ªõi fallback text extraction...")
                return {
                    "error": "No entities extracted - retry with fallback",
                    "retry_count_analyze": retry_count + 1,
                    "messages": []  # Clear messages
                }
            
            # Strategy 2: Gi·∫£m chunk size (retry 1 -> 2)  
            elif retry_count == 1:
                logging.info("üîÑ Strategy 2: Th·ª≠ l·∫°i v·ªõi chunk size nh·ªè h∆°n...")
                return {
                    "error": "No entities after fallback - retry with smaller chunks", 
                    "retry_count_analyze": retry_count + 1,
                    "messages": [{
                        "to": "agent_a2",
                        "action": "adjust_chunk",
                        "chunk_size": 1200,  # Nh·ªè h∆°n n·ªØa
                        "chunk_overlap": 100
                    }]
                }
            
            # Strategy 3: Summary-only mode v·ªõi smart fallback (retry 2+)
            else:
                logging.warning("‚ö†Ô∏è Strategy 3: Chuy·ªÉn sang summary-only mode")
                
                # T·∫°o final summary tr∆∞·ªõc
                llm = get_global_llm()
                try:
                    if summaries:
                        # K·∫øt h·ª£p summaries th√†nh final summary
                        combined_summary = "\n".join(summaries[:5])  # L·∫•y t·ªëi ƒëa 5 summaries
                        final_summary_result = llm.invoke(final_summarize_prompt.format(summaries=combined_summary))
                        final_summary = final_summary_result.content if hasattr(final_summary_result, "content") else str(final_summary_result)
                    else:
                        final_summary = "T√†i li·ªáu ch·ª©a th√¥ng tin nh∆∞ng kh√¥ng th·ªÉ tr√≠ch xu·∫•t chi ti·∫øt c·ª• th·ªÉ."
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è L·ªói t·∫°o final summary: {str(e)}")
                    final_summary = "\n".join(summaries[:3]) if summaries else "Kh√¥ng th·ªÉ t·∫°o t√≥m t·∫Øt"
                
                # T·∫°o entities t·ª´ summary (last attempt)
                try:
                    summary_entities_result = llm.invoke(fallback_extract_prompt.format(text=final_summary))
                    summary_entities_content = summary_entities_result.content if hasattr(summary_entities_result, "content") else str(summary_entities_result)
                    summary_entities = extract_entities_from_text(summary_entities_content)
                    
                    # Ki·ªÉm tra n·∫øu c√≥ entities t·ª´ summary
                    summary_entities_count = sum(len(v) for v in summary_entities.values())
                    if summary_entities_count > 0:
                        logging.info(f"‚úì T√¨m th·∫•y {summary_entities_count} entities t·ª´ final summary")
                        final_entities = summary_entities
                    else:
                        # T·∫°o entities placeholder  
                        final_entities = {
                            "names": ["T√†i li·ªáu"],
                            "dates": ["Kh√¥ng x√°c ƒë·ªãnh"],
                            "locations": ["Kh√¥ng x√°c ƒë·ªãnh"], 
                            "numbers": ["Kh√¥ng x√°c ƒë·ªãnh"]
                        }
                        
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ extract t·ª´ summary: {str(e)}")
                    # Final fallback entities
                    final_entities = {
                        "names": ["T√†i li·ªáu"],
                        "dates": ["Kh√¥ng x√°c ƒë·ªãnh"],
                        "locations": ["Kh√¥ng x√°c ƒë·ªãnh"],
                        "numbers": ["Kh√¥ng x√°c ƒë·ªãnh"]
                    }
                
                logging.info(f"‚úÖ Summary-only mode: summary={len(final_summary)} chars, entities={sum(len(v) for v in final_entities.values())} items")
                
                return {
                    "summary": final_summary,
                    "entities": final_entities,
                    "retry_count_analyze": retry_count + 1,
                    "error": None,  # Kh√¥ng l·ªói ƒë·ªÉ ti·∫øp t·ª•c workflow
                    "summary_only_mode": True,
                    "messages": []  # Clear messages
                }
        
        if not summaries:
            logging.error("‚ùå Agent Analyze: Kh√¥ng c√≥ summary n√†o ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng")
            return {
                "error": "No summaries generated",
                "retry_count_analyze": retry_count + 1,
                "messages": []  # Clear messages
            }
            
        # Lo·∫°i b·ªè duplicates v√† chu·∫©n h√≥a
        for key in entities:
            entities[key] = list(set(entities[key]))
            # Lo·∫°i b·ªè c√°c gi√° tr·ªã r·ªóng ho·∫∑c qu√° ng·∫Øn
            entities[key] = [e for e in entities[key] if isinstance(e, str) and len(e.strip()) > 2]
        
        # T·∫°o final summary
        llm = get_global_llm()
        try:
            summary_chunks = chunk_summaries_for_final(summaries, max_tokens=150000)
            logging.info(f"üìù Chia {len(summaries)} summaries th√†nh {len(summary_chunks)} chunks ƒë·ªÉ x·ª≠ l√Ω")
            
            if len(summary_chunks) == 1:
                final_summary_result = llm.invoke(final_summarize_prompt.format(summaries="\n".join(summary_chunks[0])))
                final_summary = final_summary_result.content if hasattr(final_summary_result, "content") else str(final_summary_result)
            else:
                chunk_summaries = []
                with ThreadPoolExecutor(max_workers=min(5, len(summary_chunks))) as executor:
                    chunk_futures = {
                        executor.submit(process_summary_chunk, (i, chunk)): i 
                        for i, chunk in enumerate(summary_chunks)
                    }
                    
                    for future in as_completed(chunk_futures):
                        try:
                            result = future.result(timeout=60)
                            if result and isinstance(result, str):
                                chunk_summaries.append(result)
                        except Exception as e:
                            logging.warning(f"‚ö†Ô∏è Timeout in summary processing: {str(e)}")
                
                if len(chunk_summaries) > 1:
                    try:
                        time.sleep(0.5)
                        final_result = llm.invoke(final_summarize_prompt.format(summaries="\n".join(chunk_summaries)))
                        final_summary = final_result.content if hasattr(final_result, "content") else str(final_result)
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è L·ªói khi combine final summary: {str(e)}")
                        final_summary = "\n\n".join(chunk_summaries[:3])  # L·∫•y 3 summary ƒë·∫ßu l√†m fallback
                else:
                    final_summary = chunk_summaries[0] if chunk_summaries else summaries[0]
                    
        except Exception as e:
            logging.error(f"‚ùå L·ªói khi t·∫°o final summary: {str(e)}")
            final_summary = "\n".join(summaries[:3])  # Fallback v·ªõi 3 summary ƒë·∫ßu
        
        # Log k·∫øt qu·∫£ chi ti·∫øt
        logging.info(f"""‚úÖ Agent Analyze: Ph√¢n t√≠ch th√†nh c√¥ng ({'fallback' if use_fallback else 'standard'} mode):
        - {len(summaries)}/{len(chunks)} chunks x·ª≠ l√Ω th√†nh c√¥ng
        - {len(failed_chunks)} chunks th·∫•t b·∫°i
        - {len(entities['names'])} t√™n
        - {len(entities['dates'])} ng√†y th√°ng  
        - {len(entities['locations'])} ƒë·ªãa ƒëi·ªÉm
        - {len(entities['numbers'])} s·ªë li·ªáu
        - Final summary: {len(final_summary)} k√Ω t·ª±""")
        
        if failed_chunks:
            logging.warning(f"‚ö†Ô∏è {len(failed_chunks)} chunks kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c, c√≥ th·ªÉ thi·∫øu th√¥ng tin")
        
        return {
            "summary": final_summary,
            "entities": entities,
            "retry_count_analyze": retry_count + 1,
            "error": None,
            "messages": []  # Clear messages
        }
    except Exception as e:
        logging.error(f"‚ùå Agent Analyze: L·ªói khi ph√¢n t√≠ch - {str(e)}")
        return {
            "error": str(e),
            "retry_count_analyze": retry_count + 1,
            "messages": []  # Clear messages
        }

verify_prompt = ChatPromptTemplate.from_messages([
    ("system", """X√°c minh t√≠nh ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß c·ªßa c√°c th·ª±c th·ªÉ (entities) ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t d·ª±a tr√™n c√¢u h·ªèi v√† t√≥m t·∫Øt n·ªôi dung.

C√¢u h·ªèi g·ªëc: {question}
T√≥m t·∫Øt: {summary}

Nhi·ªám v·ª• c·ªßa b·∫°n:
1. Ki·ªÉm tra xem c√°c entities c√≥ li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ ch√≠nh kh√¥ng
2. X√°c minh t√≠nh ch√≠nh x√°c c·ªßa c√°c entities (t√™n, ng√†y th√°ng, ƒë·ªãa ƒëi·ªÉm, s·ªë li·ªáu)
3. ƒê√°nh gi√° m·ª©c ƒë·ªô ƒë·∫ßy ƒë·ªß c·ªßa th√¥ng tin ƒë√£ tr√≠ch xu·∫•t
4. Ph√°t hi·ªán c√°c th√¥ng tin quan tr·ªçng b·ªã b·ªè s√≥t
5. So s√°nh entities t·ª´ search v·ªõi entities t·ª´ analysis

N·∫øu ph√°t hi·ªán v·∫•n ƒë·ªÅ, g·ª≠i message y√™u c·∫ßu x·ª≠ l√Ω l·∫°i (v√≠ d·ª•: {{"to": "agent_a1", "action": "retry_clean"}}).
Tr·∫£ v·ªÅ JSON: {{"verified": bool, "verified_data": dict, "message": dict}}"""),
    ("user", "Entities t·ª´ search: {search_entities}\nEntities t·ª´ analysis: {analysis_entities}")
])

def verified_agent(state: AgentState) -> AgentState:
    """Agent x√°c minh th√¥ng tin"""
    retry_count = state.get("retry_count_verify", 0)
    logging.info(f"üîç Agent Verify: B·∫Øt ƒë·∫ßu qu√° tr√¨nh x√°c minh (retry: {retry_count})...")
    
    if not state.api_key or not state.model_name:
        logging.error("‚ùå Agent Verify: Thi·∫øu API key ho·∫∑c model name")
        return {"error": "API key and model name are required", "messages": state.get("messages", [])}
        
    if state.get("error") or not state.get("entities") or not state.get("db"):
        error_msg = state.get("error") or "Missing data"
        logging.error(f"‚ùå Agent Verify: Thi·∫øu d·ªØ li·ªáu ƒë·ªÉ x√°c minh - {error_msg}")
        return {
            "error": error_msg, 
            "retry_count_verify": retry_count + 1,
            "messages": []  # Clear messages
        }
    
    # Ki·ªÉm tra summary-only mode
    summary_only_mode = state.get("summary_only_mode", False)
    if summary_only_mode:
        logging.info("üîç Agent Verify: X·ª≠ l√Ω summary-only mode")
    
    try:
        # S·ª≠ d·ª•ng summary t·ª´ analysis_agent l√†m query thay v√¨ c√¢u h·ªèi g·ªëc
        search_query = state.get("summary", state["question"])
        logging.info(f"üîç Agent Verify: T√¨m ki·∫øm v·ªõi query d√†i {len(search_query)} k√Ω t·ª±")
        
        # TƒÉng k l√™n ƒë·ªÉ c√≥ nhi·ªÅu k·∫øt qu·∫£ h∆°n
        result = search_tool.invoke({
            "faiss_index": state["db"],
            "query": search_query,
            "chunks": state["chunks"],
            "api_key": state.api_key,
            "embedding_model": state.embedding_model,
            "k": 5
        })
        
        if not result["entities"]["results"]:
            logging.warning("‚ö†Ô∏è Agent Verify: Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p, th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi g·ªëc")
            # Fallback v·ªÅ c√¢u h·ªèi g·ªëc n·∫øu t√¨m b·∫±ng summary kh√¥ng c√≥ k·∫øt qu·∫£
            result = search_tool.invoke({
                "faiss_index": state["db"],
                "query": state["question"],
                "chunks": state["chunks"],
                "api_key": state.api_key,
                "embedding_model": state.embedding_model,
                "k": 5
            })
        
        logging.info("‚úì Agent Verify: ƒê√£ t√¨m ki·∫øm xong v·ªõi search tool")
        
        # X·ª≠ l√Ω k·∫øt qu·∫£ t√¨m ki·∫øm v·ªõi ng∆∞·ª°ng th·∫•p h∆°n cho summary-only mode
        if not result["entities"]["results"]:
            if summary_only_mode:
                logging.warning("‚ö†Ô∏è Summary-only mode: Kh√¥ng c√≥ k·∫øt qu·∫£ search, ti·∫øp t·ª•c v·ªõi verified_data t·ª´ entities")
                # Trong summary-only mode, ch·∫•p nh·∫≠n entities hi·ªán c√≥
                verified_data = {
                    "verified_entities": state["entities"],
                    "confidence": "low", 
                    "mode": "summary_only",
                    "note": "Verified based on summary analysis only"
                }
                
                return {
                    "verified_data": verified_data,
                    "retry_count_verify": retry_count + 1,
                    "messages": [],  # Clear messages
                    "error": None
                }
            else:
                logging.error("‚ùå Agent Verify: Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o ph√π h·ª£p")
                return {
                    "error": "No matching results found",
                    "retry_count_verify": retry_count + 1,
                    "messages": [{"to": "agent_analyze", "action": "reanalyze"}]
                }
            
        # L·∫•y score trung b√¨nh c·ªßa k·∫øt qu·∫£
        avg_score = sum(result["entities"]["scores"]) / len(result["entities"]["scores"])
        
        # ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng score d·ª±a tr√™n mode
        min_score = 0.2 if summary_only_mode else 0.3
        
        if avg_score < min_score:
            if summary_only_mode:
                logging.warning(f"‚ö†Ô∏è Summary-only mode: Score th·∫•p ({avg_score:.3f}) nh∆∞ng ti·∫øp t·ª•c")
            else:
                logging.warning(f"‚ö†Ô∏è Agent Verify: Score trung b√¨nh ({avg_score:.3f}) qu√° th·∫•p")
                return {
                    "error": "Low confidence in search results",
                    "retry_count_verify": retry_count + 1,
                    "messages": [{"to": "agent_analyze", "action": "reanalyze"}]
                }
        
        llm = get_llm(state.api_key, state.model_name)
        logging.info("‚úì Agent Verify: ƒê√£ kh·ªüi t·∫°o LLM")
            
        response = llm.invoke(verify_prompt.format(
            question=state["question"],
            summary=state["summary"],
            search_entities=str({
                "results": result["entities"]["results"],
                "scores": result["entities"]["scores"]
            }),
            analysis_entities=str(state["entities"])
        ))
        logging.info("‚úì Agent Verify: ƒê√£ g·ªçi LLM ƒë·ªÉ x√°c minh")
        
        try:
            parsed = json.loads(response.content)
        except json.JSONDecodeError:
            logging.warning("‚ùå Agent Verify: Kh√¥ng th·ªÉ parse k·∫øt qu·∫£ t·ª´ LLM, t·∫°o fallback response")
            # Fallback verification cho summary-only mode
            if summary_only_mode:
                parsed = {
                    "verified": True,
                    "verified_data": {
                        "entities": state["entities"],
                        "confidence": "medium",
                        "mode": "summary_only_fallback"
                    },
                    "message": None
                }
            else:
                return {
                    "error": "Invalid LLM response format",
                    "retry_count_verify": retry_count + 1,
                    "messages": []
                }
        
        if parsed["verified"]:
            # T√≠nh s·ªë l∆∞·ª£ng entities t·ª´ m·ªói ngu·ªìn
            search_entities = result.get("entities", {})
            analysis_entities = state.get("entities", {})
            
            search_count = len(search_entities.get("results", []))
            analysis_count = sum(len(entities) for entities in analysis_entities.values())
            
            mode_info = f" (summary-only mode)" if summary_only_mode else ""
            
            logging.info(f"""‚úÖ Agent Verify: X√°c minh th√†nh c√¥ng{mode_info}
            - Entities t·ª´ search: {search_count} items (avg score: {avg_score:.3f})
            - Entities t·ª´ analysis: {analysis_count} items
            - Verified data: {len(parsed.get('verified_data', {}))} items
            - Categories: {', '.join(parsed.get('verified_data', {}).keys())}""")
            
            return {
                "verified_data": parsed["verified_data"],
                "retry_count_verify": retry_count + 1,
                "messages": [],  # Clear messages
                "error": None
            }
        else:
            logging.warning("‚ö†Ô∏è Agent Verify: X√°c minh th·∫•t b·∫°i, c·∫ßn ph√¢n t√≠ch l·∫°i")
            return {
                "error": "Verification failed",
                "retry_count_verify": retry_count + 1,
                "messages": [{"to": "agent_analyze", "action": "reanalyze"}]
            }
            
    except Exception as e:
        logging.error(f"‚ùå Agent Verify: L·ªói khi x√°c minh - {str(e)}")
        return {
            "error": str(e),
            "retry_count_verify": retry_count + 1,
            "messages": [{"to": "agent_analyze", "action": "reanalyze"}]
        }


class FinalOutput(BaseModel):
    answer: str = Field(description="C√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi")
    summary: str = Field(description="T√≥m t·∫Øt n·ªôi dung")
    entities: Dict[str, Any] = Field(description="Entities tr√≠ch xu·∫•t")
    verified_data: Dict[str, Any] = Field(description="D·ªØ li·ªáu ƒë√£ x√°c minh")

def aggregated_agent(state: AgentState) -> AgentState:
    """Agent t·ªïng h·ª£p k·∫øt qu·∫£"""
    retry_count = state.get("retry_count_aggregate", 0)
    logging.info(f"üöÄ Agent Aggregate: B·∫Øt ƒë·∫ßu t·ªïng h·ª£p k·∫øt qu·∫£ (retry: {retry_count})...")
    
    if state.get("error"):
        error_msg = state.get("error")
        logging.error(f"‚ùå Agent Aggregate: Kh√¥ng th·ªÉ t·ªïng h·ª£p do l·ªói - {error_msg}")
        return {
            "report": f"Error: {error_msg}", 
            "retry_count_aggregate": retry_count + 1,
            "messages": []
        }
    
    try:
        final_result = FinalOutput(
            answer=f"Response to '{state['question']}': {state.get('verified_data', 'No verified data')}",
            summary=state.get("summary", "No summary available"),
            entities=state.get("entities", {}),
            verified_data=state.get("verified_data", {})
        )
        
        logging.info("‚úÖ Agent Aggregate: ƒê√£ t·ªïng h·ª£p th√†nh c√¥ng")
        return {
            "report": final_result.model_dump_json(), 
            "retry_count_aggregate": retry_count + 1,
            "error": None, 
            "messages": []
        }
    except Exception as e:
        logging.error(f"‚ùå Agent Aggregate: L·ªói khi t·ªïng h·ª£p - {str(e)}")
        return {
            "error": str(e),
            "retry_count_aggregate": retry_count + 1,
            "messages": []
        }

def process_summary_chunk(chunk_data):
    """Process summary chunk for parallel execution"""
    chunk_idx, chunk = chunk_data
    try:
        llm = get_global_llm()
        chunk_result = llm.invoke(final_summarize_prompt.format(summaries="\n".join(chunk)))
        return chunk_result.content if hasattr(chunk_result, "content") else str(chunk_result)
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω summary chunk {chunk_idx}: {str(e)}")
        return " ".join(chunk[:3])  # Fallback v·ªõi 3 summaries ƒë·∫ßu

# Backward compatibility
def analyze_chunk_batch(chunk: str) -> Dict:
    """Backward compatibility wrapper"""
    return analyze_chunk_batch_with_mode(chunk, use_fallback=False)

def analyze_batch_parallel(batch: List[str], max_workers: int = 10) -> List[Dict]:
    """Backward compatibility wrapper"""
    return analyze_batch_parallel_with_mode(batch, max_workers, use_fallback=False)


