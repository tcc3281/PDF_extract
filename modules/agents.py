from modules.tools import search_tool, extract_pdf, chunk_and_embed, check_chunks
from modules.states import AgentState
from langchain.prompts import ChatPromptTemplate
from typing import List, Dict, Any
import json
import logging
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from datetime import datetime

# Cáº¥u hÃ¬nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

load_dotenv()

MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4.1-mini")  # default if not set
API_KEY = os.getenv("OPENAI_API_KEY")
if not API_KEY:
    raise ValueError("OPENAI_API_KEY not found in environment variables")

llm = ChatOpenAI(
    model=MODEL_NAME,
    temperature=0,
    api_key=API_KEY
)


def extracted_agent(state: AgentState) -> AgentState:
    logging.info("ğŸš€ Agent A1: Báº¯t Ä‘áº§u trÃ­ch xuáº¥t PDF...")
    retry_count = state.get("retry_count_a1", 0)
    if retry_count >= 3:
        logging.error("âŒ Agent A1: ÄÃ£ thá»­ 3 láº§n nhÆ°ng khÃ´ng thÃ nh cÃ´ng")
        return {"error": "Invalid PDF after 3 retries", "messages": state.get("messages", [])}
    
    messages = state.get("messages", [])
    if any(msg.get("to") == "agent_a1" and msg.get("action") == "retry_clean" for msg in messages):
        logging.info("ğŸ”„ Agent A1: Thá»­ láº¡i viá»‡c lÃ m sáº¡ch vÃ  OCR...")
    
    result = extract_pdf.invoke({"pdf_path": state["file_path"]})
    
    if result["error"]:
        logging.error(f"âŒ Agent A1: Lá»—i khi trÃ­ch xuáº¥t PDF - {result['error']}")
    else:
        logging.info(f"âœ… Agent A1: ÄÃ£ trÃ­ch xuáº¥t thÃ nh cÃ´ng {len(result['cleaned_text'])} kÃ½ tá»±")
    
    return {
        "cleaned_text": result["cleaned_text"],
        "error": result["error"],
        "retry_count_a1": retry_count + 1,
        "messages": messages
    }


def chunked_and_embedded_agent(state: AgentState) -> AgentState:
    logging.info("ğŸš€ Agent A2: Báº¯t Ä‘áº§u chia nhá» vÃ  táº¡o embeddings...")
    retry_count = state.get("retry_count_a2", 0)
    if retry_count >= 3:
        logging.error("âŒ Agent A2: ÄÃ£ thá»­ 3 láº§n nhÆ°ng khÃ´ng thÃ nh cÃ´ng")
        return {"error": "Invalid chunks after 3 retries", "messages": state.get("messages", [])}
    
    if state["error"] or not state["cleaned_text"]:
        logging.error(f"âŒ Agent A2: KhÃ´ng cÃ³ text Ä‘á»ƒ xá»­ lÃ½ - {state['error'] or 'No cleaned text'}")
        return {"error": state["error"] or "No cleaned text", "messages": state.get("messages", [])}
    
    messages = state.get("messages", [])
    chunk_size, chunk_overlap = 2000, 200
    for msg in messages:
        if msg.get("to") == "agent_a2" and msg.get("action") == "adjust_chunk":
            chunk_size = int(msg.get("chunk_size", 2000))
            chunk_overlap = int(msg.get("chunk_overlap", 200))
            logging.info(f"ğŸ”„ Agent A2: Äiá»u chá»‰nh kÃ­ch thÆ°á»›c chunk={chunk_size}, overlap={chunk_overlap}")
    
    # Táº¡o file_id tá»« file_path
    file_id = Path(state["file_path"]).stem if state.get("file_path") else "unknown"
    
    result = chunk_and_embed.invoke({
        "text": state["cleaned_text"],
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "file_id": file_id
    })

    if result.get("error"):
        logging.error(f"âŒ Agent A2: Lá»—i khi táº¡o embeddings - {result['error']}")
        return {
            "error": result["error"],
            "retry_count_a2": retry_count + 1,
            "messages": messages
        }

    if not check_chunks(result["chunks"]):
        logging.warning("âš ï¸ Agent A2: Chunks khÃ´ng há»£p lá»‡, thá»­ Ä‘iá»u chá»‰nh kÃ­ch thÆ°á»›c...")
        return {
            "error": "Invalid chunks",
            "retry_count_a2": retry_count + 1,
            "messages": messages + [{"to": "agent_a2", "action": "adjust_chunk", "chunk_size": chunk_size - 500, "chunk_overlap": chunk_overlap - 50}]
        }

    logging.info(f"âœ… Agent A2: ÄÃ£ táº¡o {len(result['chunks'])} chunks vÃ  embeddings thÃ nh cÃ´ng")
    return {
        "chunks": result["chunks"],
        "embeddings": result["embeddings"],
        "db": result["db"],
        "retry_count_a2": retry_count + 1,
        "error": None,
        "messages": messages
    }


summarize_prompt = ChatPromptTemplate.from_messages([
    ("system", """Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  trÃ­ch xuáº¥t thÃ´ng tin quan trá»ng, chÃ­nh xÃ¡c vÃ  ngáº¯n gá»n tá»« vÄƒn báº£n. 
Táº­p trung vÃ o:
1. Dá»¯ kiá»‡n chÃ­nh (facts) vÃ  thÃ´ng tin cá»‘t lÃµi
2. TÃªn ngÆ°á»i, tá»• chá»©c quan trá»ng
3. Äá»‹a Ä‘iá»ƒm vÃ  thá»i gian cá»¥ thá»ƒ
4. Sá»‘ liá»‡u Ä‘á»‹nh lÆ°á»£ng vÃ  thá»‘ng kÃª
5. Má»‘i quan há»‡ giá»¯a cÃ¡c thá»±c thá»ƒ

Bá» qua:
- ThÃ´ng tin trÃ¹ng láº·p
- Chi tiáº¿t khÃ´ng quan trá»ng
- Ná»™i dung mang tÃ­nh quáº£ng cÃ¡o
- ÄÃ¡nh giÃ¡ chá»§ quan

TÃ³m táº¯t ngáº¯n gá»n, sÃºc tÃ­ch (tá»‘i Ä‘a 100 tá»«), chá»‰ giá»¯ láº¡i thÃ´ng tin quan trá»ng nháº¥t."""),
    ("user", "{text}")
])

extract_prompt = ChatPromptTemplate.from_messages([
    ("system", """TrÃ­ch xuáº¥t cÃ¡c thá»±c thá»ƒ (entities) quan trá»ng tá»« vÄƒn báº£n sau má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§.
PhÃ¢n loáº¡i thÃ nh 4 nhÃ³m:
1. names: TÃªn ngÆ°á»i, tá»• chá»©c, cÃ´ng ty, thÆ°Æ¡ng hiá»‡u quan trá»ng
2. dates: NgÃ y thÃ¡ng, má»‘c thá»i gian, khoáº£ng thá»i gian
3. locations: Äá»‹a Ä‘iá»ƒm, quá»‘c gia, thÃ nh phá»‘, khu vá»±c Ä‘á»‹a lÃ½
4. numbers: Sá»‘ liá»‡u thá»‘ng kÃª, tiá»n tá»‡, pháº§n trÄƒm, sá»‘ Ä‘o lÆ°á»ng

Chá»‰ trÃ­ch xuáº¥t cÃ¡c entities thá»±c sá»± quan trá»ng vÃ  cÃ³ giÃ¡ trá»‹ thÃ´ng tin cao.
Bá» qua cÃ¡c entities khÃ´ng rÃµ rÃ ng hoáº·c khÃ´ng quan trá»ng.
Tráº£ vá» Ä‘Ãºng Ä‘á»‹nh dáº¡ng JSON: {{"entities": {{"names": [], "dates": [], "locations": [], "numbers": []}}}}"""),
    ("user", "{text}")
])

final_summarize_prompt = ChatPromptTemplate.from_messages([
    ("system", """Tá»•ng há»£p cÃ¡c pháº§n thÃ´ng tin Ä‘Ã£ Ä‘Æ°á»£c trÃ­ch xuáº¥t thÃ nh má»™t báº£n tÃ³m táº¯t hoÃ n chá»‰nh, ngáº¯n gá»n vÃ  cÃ³ cáº¥u trÃºc.
YÃªu cáº§u:
1. Ngáº¯n gá»n, sÃºc tÃ­ch, khÃ´ng quÃ¡ 500 tá»«
2. Chá»‰ giá»¯ láº¡i thÃ´ng tin quan trá»ng vÃ  cÃ³ giÃ¡ trá»‹ cao
3. Sáº¯p xáº¿p thÃ´ng tin theo thá»© tá»± logic vÃ  dá»… hiá»ƒu
4. LiÃªn káº¿t cÃ¡c thÃ´ng tin cÃ³ liÃªn quan vá»›i nhau
5. Äáº£m báº£o chÃ­nh xÃ¡c vÃ  khÃ¡ch quan
6. Táº­p trung vÃ o dá»¯ kiá»‡n, sá»‘ liá»‡u vÃ  má»‘i quan há»‡ giá»¯a cÃ¡c thá»±c thá»ƒ
7. Loáº¡i bá» thÃ´ng tin trÃ¹ng láº·p, khÃ´ng quan trá»ng hoáº·c mang tÃ­nh chá»§ quan

Má»¥c Ä‘Ã­ch: GiÃºp ngÆ°á»i Ä‘á»c náº¯m báº¯t nhanh chÃ³ng nhá»¯ng thÃ´ng tin quan trá»ng nháº¥t tá»« vÄƒn báº£n gá»‘c."""),
    ("user", "{summaries}")
])

def analyze_chunk_batch(chunk: str) -> Dict:
    """Xá»­ lÃ½ má»™t chunk Ä‘Æ¡n láº» - Ä‘á»ƒ dÃ¹ng trong parallel processing"""
    try:
        # Xá»­ lÃ½ summary
        summary_result = llm.invoke(summarize_prompt.format(text=chunk))
        summary = summary_result.content if hasattr(summary_result, "content") else str(summary_result)
        
        # Xá»­ lÃ½ entities vá»›i error handling tá»‘t hÆ¡n
        entities_result = llm.invoke(extract_prompt.format(text=chunk))
        
        # Fix: Xá»­ lÃ½ Ä‘Ãºng ChatPromptValue vÃ  cÃ¡c Ä‘á»‹nh dáº¡ng khÃ¡c
        if hasattr(entities_result, "content"):
            entities_content = entities_result.content
        elif hasattr(entities_result, "text"):
            entities_content = entities_result.text
        else:
            entities_content = str(entities_result)
        
        # Parse JSON entities
        try:
            if isinstance(entities_content, str):
                entities_data = json.loads(entities_content)
                if "entities" in entities_data:
                    entities = entities_data["entities"]
                else:
                    entities = entities_data
            else:
                entities = {"names": [], "dates": [], "locations": [], "numbers": []}
        except (json.JSONDecodeError, KeyError) as e:
            logging.warning(f"âš ï¸ KhÃ´ng thá»ƒ parse entities JSON: {str(e)}")
            entities = {"names": [], "dates": [], "locations": [], "numbers": []}
        
        # Äáº£m báº£o entities cÃ³ Ä‘Ãºng structure
        if not isinstance(entities, dict):
            entities = {"names": [], "dates": [], "locations": [], "numbers": []}
        
        for key in ["names", "dates", "locations", "numbers"]:
            if key not in entities:
                entities[key] = []
            elif not isinstance(entities[key], list):
                entities[key] = []
        
        result = {
            "summary": summary,
            "entities": entities
        }
        
        logging.info(f"âœ… Chunk processed: {len(chunk)} chars, entities: {sum(len(v) for v in entities.values())} items")
        return result
        
    except Exception as e:
        logging.error(f"âŒ Error processing chunk: {str(e)}")
        return {
            "summary": "Error processing this chunk",
            "entities": {"names": [], "dates": [], "locations": [], "numbers": []}
        }

def analyze_batch_parallel(batch: List[str], max_workers: int = 10) -> List[Dict]:
    """Xá»­ lÃ½ batch vá»›i parallel processing Ä‘á»ƒ tÄƒng tá»‘c"""
    try:
        results = []
        
        # Sá»­ dá»¥ng ThreadPoolExecutor Ä‘á»ƒ xá»­ lÃ½ parallel
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit táº¥t cáº£ chunks trong batch
            future_to_chunk = {
                executor.submit(analyze_chunk_batch, chunk): chunk 
                for chunk in batch
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_chunk):
                try:
                    result = future.result(timeout=30)  # 30s timeout per chunk
                    if result:
                        results.append(result)
                except Exception as e:
                    logging.error(f"âŒ Timeout or error in parallel processing: {str(e)}")
                    # Add fallback result
                    results.append({
                        "summary": "Error processing chunk",
                        "entities": {"names": [], "dates": [], "locations": [], "numbers": []}
                    })
        
        return results
    except Exception as e:
        logging.error(f"âŒ Error in parallel batch processing: {str(e)}")
        return []

def count_tokens_estimate(text: str) -> int:
    """Æ¯á»›c tÃ­nh sá»‘ tokens (khoáº£ng 3.5 kÃ½ tá»± = 1 token cho tiáº¿ng Viá»‡t)"""
    return len(text) // 3

def chunk_summaries_for_final(summaries: List[str], max_tokens: int = 150000) -> List[List[str]]:
    """Chia summaries thÃ nh cÃ¡c chunks lá»›n hÆ¡n Ä‘á»ƒ táº­n dá»¥ng token limit cao"""
    chunks = []
    current_chunk = []
    current_tokens = 0
    
    for summary in summaries:
        summary_tokens = count_tokens_estimate(summary)
        
        if current_tokens + summary_tokens > max_tokens and current_chunk:
            chunks.append(current_chunk)
            current_chunk = [summary]
            current_tokens = summary_tokens
        else:
            current_chunk.append(summary)
            current_tokens += summary_tokens
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks

def analyzed_agent(state: AgentState) -> AgentState:
    logging.info("ğŸš€ Agent Analyze: Báº¯t Ä‘áº§u phÃ¢n tÃ­ch ná»™i dung vá»›i hiá»‡u suáº¥t cao...")
    retry_count = state.get("retry_count_analyze", 0)
    if retry_count >= 3:
        logging.error("âŒ Agent Analyze: ÄÃ£ thá»­ 3 láº§n nhÆ°ng khÃ´ng thÃ nh cÃ´ng")
        return {"error": "Analysis failed after 3 retries", "messages": state.get("messages", [])}
    
    if state["error"] or not state["chunks"]:
        logging.error(f"âŒ Agent Analyze: KhÃ´ng cÃ³ chunks Ä‘á»ƒ phÃ¢n tÃ­ch - {state['error'] or 'No chunks available'}")
        return {"error": state["error"] or "No chunks available", "messages": state.get("messages", [])}

    try:
        # TÄƒng batch size Ä‘á»ƒ táº­n dá»¥ng rate limit cao
        batch_size = 20  # TÄƒng tá»« 5 lÃªn 20 
        chunks = state["chunks"]
        batches = [chunks[i:i + batch_size] for i in range(0, len(chunks), batch_size)]
        logging.info(f"ğŸ“Š Agent Analyze: Xá»­ lÃ½ {len(batches)} batches vá»›i batch_size={batch_size} (parallel mode)...")

        summaries = []
        entities = {"names": [], "dates": [], "locations": [], "numbers": []}
        
        # Xá»­ lÃ½ cÃ¡c batches vá»›i parallel processing
        for i, batch in enumerate(batches):
            # Giáº£m delay xuá»‘ng chá»‰ 0.2s giá»¯a batches Ä‘á»ƒ táº­n dá»¥ng 500 RPM
            if i > 0:
                time.sleep(0.2)
            
            logging.info(f"ğŸ”„ Processing batch {i+1}/{len(batches)} vá»›i {len(batch)} chunks...")
            
            # Sá»­ dá»¥ng parallel processing cho tá»«ng batch
            batch_results = analyze_batch_parallel(batch, max_workers=min(15, len(batch)))
            
            if not batch_results:
                logging.warning(f"âš ï¸ Batch {i+1} khÃ´ng cÃ³ káº¿t quáº£")
                continue
                
            for result in batch_results:
                if not result:
                    continue
                summaries.append(result["summary"])
                for key in entities:
                    if key in result["entities"] and isinstance(result["entities"][key], list):
                        entities[key].extend(result["entities"][key])
        
        if not summaries:
            raise ValueError("KhÃ´ng cÃ³ summary nÃ o Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng")
            
        # Loáº¡i bá» duplicates
        for key in entities:
            entities[key] = list(set(entities[key]))
        
        # Optimized final summary processing vá»›i token limit cao hÆ¡n
        try:
            # Táº­n dá»¥ng token limit 200k - chia thÃ nh chunks lá»›n hÆ¡n
            summary_chunks = chunk_summaries_for_final(summaries, max_tokens=150000)
            logging.info(f"ğŸ“ Chia {len(summaries)} summaries thÃ nh {len(summary_chunks)} chunks Ä‘á»ƒ xá»­ lÃ½")
            
            if len(summary_chunks) == 1:
                # Xá»­ lÃ½ 1 chunk lá»›n
                final_summary_result = llm.invoke(final_summarize_prompt.format(summaries="\n".join(summary_chunks[0])))
                final_summary = final_summary_result.content if hasattr(final_summary_result, "content") else str(final_summary_result)
            else:
                # Xá»­ lÃ½ parallel cÃ¡c summary chunks
                chunk_summaries = []
                
                def process_summary_chunk(chunk_data):
                    chunk_idx, chunk = chunk_data
                    try:
                        chunk_result = llm.invoke(final_summarize_prompt.format(summaries="\n".join(chunk)))
                        return chunk_result.content if hasattr(chunk_result, "content") else str(chunk_result)
                    except Exception as e:
                        logging.warning(f"âš ï¸ Lá»—i khi xá»­ lÃ½ summary chunk {chunk_idx}: {str(e)}")
                        return " ".join(chunk[:5])  # Fallback vá»›i nhiá»u summaries hÆ¡n
                
                # Parallel processing cho summary chunks
                with ThreadPoolExecutor(max_workers=min(5, len(summary_chunks))) as executor:
                    chunk_futures = {
                        executor.submit(process_summary_chunk, (i, chunk)): i 
                        for i, chunk in enumerate(summary_chunks)
                    }
                    
                    for future in as_completed(chunk_futures):
                        try:
                            result = future.result(timeout=60)  # Longer timeout for summary
                            chunk_summaries.append(result)
                        except Exception as e:
                            logging.warning(f"âš ï¸ Timeout in summary processing: {str(e)}")
                            chunk_summaries.append("Summary processing failed")
                
                # Final combination
                if len(chunk_summaries) > 1:
                    try:
                        time.sleep(0.5)  # Brief delay
                        final_result = llm.invoke(final_summarize_prompt.format(summaries="\n".join(chunk_summaries)))
                        final_summary = final_result.content if hasattr(final_result, "content") else str(final_result)
                    except Exception as e:
                        logging.warning(f"âš ï¸ Lá»—i khi combine final summary: {str(e)}")
                        final_summary = "\n\n".join(chunk_summaries)
                else:
                    final_summary = chunk_summaries[0] if chunk_summaries else "KhÃ´ng thá»ƒ táº¡o summary"
                    
        except Exception as e:
            logging.error(f"âŒ Lá»—i khi táº¡o final summary: {str(e)}")
            # Fallback vá»›i nhiá»u summaries hÆ¡n
            final_summary = "\n".join(summaries[:10]) + ("..." if len(summaries) > 10 else "")
        
        # LÆ°u intermediate results vá»›i tÃªn file unique
        file_id = Path(state["file_path"]).stem if state.get("file_path") else "unknown"
        intermediate_filename = f"analyze_intermediate_{file_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        intermediate_path = Path("temp_files") / intermediate_filename
        
        with open(intermediate_path, "w", encoding='utf-8') as f:
            json.dump({
                "summaries": summaries, 
                "entities": entities,
                "final_summary": final_summary,
                "performance_stats": {
                    "total_chunks": len(chunks),
                    "total_batches": len(batches),
                    "batch_size": batch_size,
                    "total_summaries": len(summaries),
                    "total_entities": sum(len(v) for v in entities.values())
                }
            }, f, ensure_ascii=False, indent=2)
        
        logging.info(f"""âœ… Agent Analyze: PhÃ¢n tÃ­ch thÃ nh cÃ´ng (High Performance Mode):
        - {len(summaries)} summaries
        - {len(entities['names'])} tÃªn
        - {len(entities['dates'])} ngÃ y thÃ¡ng  
        - {len(entities['locations'])} Ä‘á»‹a Ä‘iá»ƒm
        - {len(entities['numbers'])} sá»‘ liá»‡u
        - Final summary: {len(final_summary)} kÃ½ tá»±
        - Processed {len(chunks)} chunks in {len(batches)} batches""")
        
        return {
            "summary": final_summary,
            "entities": entities,
            "retry_count_analyze": retry_count + 1,
            "error": None,
            "messages": state.get("messages", [])
        }
    except Exception as e:
        logging.error(f"âŒ Agent Analyze: Lá»—i khi phÃ¢n tÃ­ch - {str(e)}")
        return {
            "error": str(e),
            "retry_count_analyze": retry_count + 1,
            "messages": state.get("messages", [])
        }

verify_prompt = ChatPromptTemplate.from_messages([
    ("system", """XÃ¡c minh tÃ­nh chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§ cá»§a cÃ¡c thá»±c thá»ƒ (entities) Ä‘Ã£ Ä‘Æ°á»£c trÃ­ch xuáº¥t.
Nhiá»‡m vá»¥ cá»§a báº¡n:
1. Kiá»ƒm tra xem cÃ¡c entities cÃ³ liÃªn quan Ä‘áº¿n chá»§ Ä‘á» chÃ­nh khÃ´ng
2. XÃ¡c minh tÃ­nh chÃ­nh xÃ¡c cá»§a cÃ¡c entities (tÃªn, ngÃ y thÃ¡ng, Ä‘á»‹a Ä‘iá»ƒm, sá»‘ liá»‡u)
3. ÄÃ¡nh giÃ¡ má»©c Ä‘á»™ Ä‘áº§y Ä‘á»§ cá»§a thÃ´ng tin Ä‘Ã£ trÃ­ch xuáº¥t
4. PhÃ¡t hiá»‡n cÃ¡c thÃ´ng tin quan trá»ng bá»‹ bá» sÃ³t

Náº¿u phÃ¡t hiá»‡n váº¥n Ä‘á», gá»­i message yÃªu cáº§u xá»­ lÃ½ láº¡i (vÃ­ dá»¥: {{"to": "agent_a1", "action": "retry_clean"}}).
Tráº£ vá» JSON: {{"verified": bool, "verified_data": dict, "message": dict}}"""),
    ("user", "Entities: {entities}")
])

def verified_agent(state: AgentState) -> AgentState:
    logging.info("ğŸš€ Agent Verify: Báº¯t Ä‘áº§u xÃ¡c minh káº¿t quáº£...")
    if state["error"] or not state["entities"] or not state["db"]:
        logging.error(f"âŒ Agent Verify: Thiáº¿u dá»¯ liá»‡u Ä‘á»ƒ xÃ¡c minh - {state['error'] or 'Missing data'}")
        return {"error": state["error"] or "Missing data", "messages": state.get("messages", [])}
    
    try:
        result = search_tool.invoke({
            "faiss_index": state["db"],
            "query": state["question"],
            "chunks": state["chunks"]
        })
        
        response = llm.invoke(verify_prompt.format(
            question=state["question"],
            summary=state["summary"],
            entities=str(result["entities"])
        ))
        
        parsed = json.loads(response.content)
        messages = state.get("messages", []) + [parsed["message"]] if parsed["message"] else state.get("messages", [])
        
        if parsed["verified"]:
            logging.info("âœ… Agent Verify: XÃ¡c minh thÃ nh cÃ´ng")
        else:
            logging.warning("âš ï¸ Agent Verify: XÃ¡c minh tháº¥t báº¡i, cáº§n phÃ¢n tÃ­ch láº¡i")
            
        return {
            "verified_data": parsed["verified_data"],
            "messages": messages,
            "error": None if parsed["verified"] else "Verification failed"
        }
    except Exception as e:
        logging.error(f"âŒ Agent Verify: Lá»—i khi xÃ¡c minh - {str(e)}")
        return {"error": str(e), "messages": state.get("messages", [])}


from pydantic import BaseModel, Field
class FinalOutput(BaseModel):
    answer: str = Field(description="CÃ¢u tráº£ lá»i cho cÃ¢u há»i")
    summary: str = Field(description="TÃ³m táº¯t ná»™i dung")
    entities: Dict[str, Any] = Field(description="Entities trÃ­ch xuáº¥t")
    verified_data: Dict[str, Any] = Field(description="Dá»¯ liá»‡u Ä‘Ã£ xÃ¡c minh")

def aggregated_agent(state: AgentState) -> AgentState:
    logging.info("ğŸš€ Agent Aggregate: Báº¯t Ä‘áº§u tá»•ng há»£p káº¿t quáº£...")
    if state["error"]:
        logging.error(f"âŒ Agent Aggregate: KhÃ´ng thá»ƒ tá»•ng há»£p do lá»—i - {state['error']}")
        return {"report": f"Error: {state['error']}", "messages": state.get("messages", [])}
    
    try:
        final_result = FinalOutput(
            answer=f"Response to '{state['question']}': {state['verified_data']}",
            summary=state["summary"],
            entities=state["entities"],
            verified_data=state["verified_data"]
        )
        
        logging.info("âœ… Agent Aggregate: ÄÃ£ tá»•ng há»£p thÃ nh cÃ´ng")
        return {"report": final_result.model_dump_json(), "error": None, "messages": state.get("messages", [])}
    except Exception as e:
        logging.error(f"âŒ Agent Aggregate: Lá»—i khi tá»•ng há»£p - {str(e)}")
        return {
            "error": str(e),
            "messages": state.get("messages", []) + [{"to": "agent_verify", "action": "reverify"}]
        }


