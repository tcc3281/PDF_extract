from modules.tools import search_tool, extract_pdf, chunk_and_embed, check_chunks
from modules.states import AgentState
from langchain.prompts import ChatPromptTemplate
from typing import List, Dict, Any
import json
import logging
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from datetime import datetime

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

load_dotenv()

def get_llm(api_key: str, model_name: str) -> ChatOpenAI:
    """Kh·ªüi t·∫°o LLM v·ªõi API key v√† model ƒë∆∞·ª£c truy·ªÅn v√†o"""
    return ChatOpenAI(
        model=model_name,
        temperature=0,
        api_key=api_key
    )

def extracted_agent(state: AgentState) -> AgentState:
    logging.info("üöÄ Agent A1: B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t PDF...")
    retry_count = state.get("retry_count_a1", 0)
    if retry_count >= 3:
        logging.error("‚ùå Agent A1: ƒê√£ th·ª≠ 3 l·∫ßn nh∆∞ng kh√¥ng th√†nh c√¥ng")
        return {"error": "Invalid PDF after 3 retries", "messages": state.get("messages", [])}
    
    messages = state.get("messages", [])
    if any(msg.get("to") == "agent_a1" and msg.get("action") == "retry_clean" for msg in messages):
        logging.info("üîÑ Agent A1: Th·ª≠ l·∫°i vi·ªác l√†m s·∫°ch v√† OCR...")
    
    result = extract_pdf.invoke({"pdf_path": state["file_path"]})
    
    if result["error"]:
        logging.error(f"‚ùå Agent A1: L·ªói khi tr√≠ch xu·∫•t PDF - {result['error']}")
    else:
        logging.info(f"‚úÖ Agent A1: ƒê√£ tr√≠ch xu·∫•t th√†nh c√¥ng {len(result['cleaned_text'])} k√Ω t·ª±")
    
    return {
        "cleaned_text": result["cleaned_text"],
        "error": result["error"],
        "retry_count_a1": retry_count + 1,
        "messages": messages
    }


def chunked_and_embedded_agent(state: AgentState) -> AgentState:
    """Agent ph√¢n ƒëo·∫°n v√† t·∫°o embeddings"""
    logging.info("üöÄ Agent A2: B·∫Øt ƒë·∫ßu chia nh·ªè v√† t·∫°o embeddings...")
    retry_count = state.get("retry_count_a2", 0)
    if retry_count >= 3:
        logging.error("‚ùå Agent A2: ƒê√£ th·ª≠ 3 l·∫ßn nh∆∞ng kh√¥ng th√†nh c√¥ng")
        return {"error": "Invalid chunks after 3 retries", "messages": state.get("messages", [])}
    
    if state["error"] or not state["cleaned_text"]:
        logging.error(f"‚ùå Agent A2: Kh√¥ng c√≥ text ƒë·ªÉ x·ª≠ l√Ω - {state['error'] or 'No cleaned text'}")
        return {"error": state["error"] or "No cleaned text", "messages": state.get("messages", [])}
    
    messages = state.get("messages", [])
    chunk_size, chunk_overlap = 2000, 200
    for msg in messages:
        if msg.get("to") == "agent_a2" and msg.get("action") == "adjust_chunk":
            chunk_size = int(msg.get("chunk_size", 2000))
            chunk_overlap = int(msg.get("chunk_overlap", 200))
            logging.info(f"üîÑ Agent A2: ƒêi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc chunk={chunk_size}, overlap={chunk_overlap}")
    
    # T·∫°o file_id t·ª´ file_path
    file_id = Path(state["file_path"]).stem if state.get("file_path") else "unknown"
    
    result = chunk_and_embed.invoke({
        "text": state["cleaned_text"],
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "file_id": file_id,
        "api_key": state.api_key,
        "embedding_model": state.embedding_model
    })

    if result.get("error"):
        logging.error(f"‚ùå Agent A2: L·ªói khi t·∫°o embeddings - {result['error']}")
        return {
            "error": result["error"],
            "retry_count_a2": retry_count + 1,
            "messages": messages
        }

    if not check_chunks(result["chunks"]):
        logging.warning("‚ö†Ô∏è Agent A2: Chunks kh√¥ng h·ª£p l·ªá, th·ª≠ ƒëi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc...")
        return {
            "error": "Invalid chunks",
            "retry_count_a2": retry_count + 1,
            "messages": messages + [{"to": "agent_a2", "action": "adjust_chunk", "chunk_size": chunk_size - 500, "chunk_overlap": chunk_overlap - 50}]
        }

    logging.info(f"‚úÖ Agent A2: ƒê√£ t·∫°o {len(result['chunks'])} chunks v√† embeddings th√†nh c√¥ng")
    return {
        "chunks": result["chunks"],
        "embeddings": result["embeddings"],
        "db": result["db"],
        "retry_count_a2": retry_count + 1,
        "error": None,
        "messages": messages
    }


summarize_prompt = ChatPromptTemplate.from_messages([
    ("system", """Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr√≠ch xu·∫•t th√¥ng tin quan tr·ªçng, ch√≠nh x√°c v√† ng·∫Øn g·ªçn t·ª´ vƒÉn b·∫£n. 
T·∫≠p trung v√†o:
1. D·ªØ ki·ªán ch√≠nh (facts) v√† th√¥ng tin c·ªët l√µi
2. T√™n ng∆∞·ªùi, t·ªï ch·ª©c quan tr·ªçng
3. ƒê·ªãa ƒëi·ªÉm v√† th·ªùi gian c·ª• th·ªÉ
4. S·ªë li·ªáu ƒë·ªãnh l∆∞·ª£ng v√† th·ªëng k√™
5. M·ªëi quan h·ªá gi·ªØa c√°c th·ª±c th·ªÉ

B·ªè qua:
- Th√¥ng tin tr√πng l·∫∑p
- Chi ti·∫øt kh√¥ng quan tr·ªçng
- N·ªôi dung mang t√≠nh qu·∫£ng c√°o
- ƒê√°nh gi√° ch·ªß quan

T√≥m t·∫Øt ng·∫Øn g·ªçn, s√∫c t√≠ch (t·ªëi ƒëa 100 t·ª´), ch·ªâ gi·ªØ l·∫°i th√¥ng tin quan tr·ªçng nh·∫•t."""),
    ("user", "{text}")
])

extract_prompt = ChatPromptTemplate.from_messages([
    ("system", """Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ (entities) quan tr·ªçng t·ª´ vƒÉn b·∫£n sau m·ªôt c√°ch ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß.
Ph√¢n lo·∫°i th√†nh 4 nh√≥m:
1. names: T√™n ng∆∞·ªùi, t·ªï ch·ª©c, c√¥ng ty, th∆∞∆°ng hi·ªáu quan tr·ªçng
2. dates: Ng√†y th√°ng, m·ªëc th·ªùi gian, kho·∫£ng th·ªùi gian
3. locations: ƒê·ªãa ƒëi·ªÉm, qu·ªëc gia, th√†nh ph·ªë, khu v·ª±c ƒë·ªãa l√Ω
4. numbers: S·ªë li·ªáu th·ªëng k√™, ti·ªÅn t·ªá, ph·∫ßn trƒÉm, s·ªë ƒëo l∆∞·ªùng

Ch·ªâ tr√≠ch xu·∫•t c√°c entities th·ª±c s·ª± quan tr·ªçng v√† c√≥ gi√° tr·ªã th√¥ng tin cao.
B·ªè qua c√°c entities kh√¥ng r√µ r√†ng ho·∫∑c kh√¥ng quan tr·ªçng.
Tr·∫£ v·ªÅ ƒë√∫ng ƒë·ªãnh d·∫°ng JSON: {{"entities": {{"names": [], "dates": [], "locations": [], "numbers": []}}}}"""),
    ("user", "{text}")
])

final_summarize_prompt = ChatPromptTemplate.from_messages([
    ("system", """T·ªïng h·ª£p c√°c ph·∫ßn th√¥ng tin ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t th√†nh m·ªôt b·∫£n t√≥m t·∫Øt ho√†n ch·ªânh, ng·∫Øn g·ªçn v√† c√≥ c·∫•u tr√∫c.
Y√™u c·∫ßu:
1. Ng·∫Øn g·ªçn, s√∫c t√≠ch, kh√¥ng qu√° 500 t·ª´
2. Ch·ªâ gi·ªØ l·∫°i th√¥ng tin quan tr·ªçng v√† c√≥ gi√° tr·ªã cao
3. S·∫Øp x·∫øp th√¥ng tin theo th·ª© t·ª± logic v√† d·ªÖ hi·ªÉu
4. Li√™n k·∫øt c√°c th√¥ng tin c√≥ li√™n quan v·ªõi nhau
5. ƒê·∫£m b·∫£o ch√≠nh x√°c v√† kh√°ch quan
6. T·∫≠p trung v√†o d·ªØ ki·ªán, s·ªë li·ªáu v√† m·ªëi quan h·ªá gi·ªØa c√°c th·ª±c th·ªÉ
7. Lo·∫°i b·ªè th√¥ng tin tr√πng l·∫∑p, kh√¥ng quan tr·ªçng ho·∫∑c mang t√≠nh ch·ªß quan

M·ª•c ƒë√≠ch: Gi√∫p ng∆∞·ªùi ƒë·ªçc n·∫Øm b·∫Øt nhanh ch√≥ng nh·ªØng th√¥ng tin quan tr·ªçng nh·∫•t t·ª´ vƒÉn b·∫£n g·ªëc."""),
    ("user", "{summaries}")
])

def analyze_chunk_batch(chunk: str, api_key: str, model_name: str) -> Dict:
    """X·ª≠ l√Ω m·ªôt chunk ƒë∆°n l·∫ª - ƒë·ªÉ d√πng trong parallel processing"""
    try:
        # Kh·ªüi t·∫°o LLM v·ªõi API key v√† model ƒë∆∞·ª£c truy·ªÅn v√†o
        llm = get_llm(api_key, model_name)
        
        # X·ª≠ l√Ω summary
        summary_result = llm.invoke(summarize_prompt.format(text=chunk))
        summary = summary_result.content if hasattr(summary_result, "content") else str(summary_result)
        
        # X·ª≠ l√Ω entities v·ªõi error handling t·ªët h∆°n
        entities_result = llm.invoke(extract_prompt.format(text=chunk))
        
        # Fix: X·ª≠ l√Ω ƒë√∫ng ChatPromptValue v√† c√°c ƒë·ªãnh d·∫°ng kh√°c
        if hasattr(entities_result, "content"):
            entities_content = entities_result.content
        elif hasattr(entities_result, "text"):
            entities_content = entities_result.text
        else:
            entities_content = str(entities_result)
        
        # Parse JSON entities
        try:
            if isinstance(entities_content, str):
                entities_data = json.loads(entities_content)
                if "entities" in entities_data:
                    entities = entities_data["entities"]
                else:
                    entities = entities_data
            else:
                entities = {"names": [], "dates": [], "locations": [], "numbers": []}
        except (json.JSONDecodeError, KeyError) as e:
            logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ parse entities JSON: {str(e)}")
            entities = {"names": [], "dates": [], "locations": [], "numbers": []}
        
        # ƒê·∫£m b·∫£o entities c√≥ ƒë√∫ng structure
        if not isinstance(entities, dict):
            entities = {"names": [], "dates": [], "locations": [], "numbers": []}
        
        for key in ["names", "dates", "locations", "numbers"]:
            if key not in entities:
                entities[key] = []
            elif not isinstance(entities[key], list):
                entities[key] = []
        
        result = {
            "summary": summary,
            "entities": entities
        }
        
        logging.info(f"‚úÖ Chunk processed: {len(chunk)} chars, entities: {sum(len(v) for v in entities.values())} items")
        return result
        
    except Exception as e:
        logging.error(f"‚ùå Error processing chunk: {str(e)}")
        return {
            "summary": "Error processing this chunk",
            "entities": {"names": [], "dates": [], "locations": [], "numbers": []}
        }

def analyze_batch_parallel(batch: List[str], api_key: str, model_name: str, max_workers: int = 10) -> List[Dict]:
    """X·ª≠ l√Ω song song c√°c chunks"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(analyze_chunk_batch, chunk, api_key, model_name) for chunk in batch]
        results = []
        for future in as_completed(futures):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                logging.error(f"‚ùå L·ªói khi x·ª≠ l√Ω chunk: {str(e)}")
                results.append({"summary": "", "entities": {"names": [], "dates": [], "locations": [], "numbers": []}})
    return results

def count_tokens_estimate(text: str) -> int:
    """∆Ø·ªõc t√≠nh s·ªë tokens (kho·∫£ng 3.5 k√Ω t·ª± = 1 token cho ti·∫øng Vi·ªát)"""
    return len(text) // 3

def chunk_summaries_for_final(summaries: List[str], max_tokens: int = 150000) -> List[List[str]]:
    """Chia summaries th√†nh c√°c chunks l·ªõn h∆°n ƒë·ªÉ t·∫≠n d·ª•ng token limit cao"""
    chunks = []
    current_chunk = []
    current_tokens = 0
    
    for summary in summaries:
        summary_tokens = count_tokens_estimate(summary)
        
        if current_tokens + summary_tokens > max_tokens and current_chunk:
            chunks.append(current_chunk)
            current_chunk = [summary]
            current_tokens = summary_tokens
        else:
            current_chunk.append(summary)
            current_tokens += summary_tokens
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks

def analyzed_agent(state: AgentState) -> AgentState:
    """Agent ph√¢n t√≠ch n·ªôi dung"""
    if not state.api_key or not state.model_name:
        return {"error": "API key and model name are required", "messages": state.get("messages", [])}
        
    logging.info("üöÄ Agent A3: B·∫Øt ƒë·∫ßu ph√¢n t√≠ch n·ªôi dung...")
    retry_count = state.get("retry_count_analyze", 0)
    if retry_count >= 3:
        logging.error("‚ùå Agent A3: ƒê√£ th·ª≠ 3 l·∫ßn nh∆∞ng kh√¥ng th√†nh c√¥ng")
        return {"error": "Analysis failed after 3 retries", "messages": state.get("messages", [])}
    
    if state["error"] or not state["chunks"]:
        logging.error(f"‚ùå Agent A3: Kh√¥ng c√≥ chunks ƒë·ªÉ ph√¢n t√≠ch - {state['error'] or 'No chunks'}")
        return {"error": state["error"] or "No chunks", "messages": state.get("messages", [])}
    
    try:
        # X·ª≠ l√Ω song song c√°c chunks
        results = analyze_batch_parallel(state["chunks"], state.api_key, state.model_name)
        
        # T·ªïng h·ª£p k·∫øt qu·∫£
        all_summaries = []
        all_entities = {"names": set(), "dates": set(), "locations": set(), "numbers": set()}
        
        for result in results:
            if "summary" in result:
                all_summaries.append(result["summary"])
            
            if "entities" in result:
                entities = result["entities"]
                for key in all_entities:
                    if key in entities:
                        all_entities[key].update(entities[key])
        
        # Chuy·ªÉn set th√†nh list
        final_entities = {k: sorted(list(v)) for k, v in all_entities.items()}
        
        # T·ªïng h·ª£p summary cu·ªëi c√πng
        llm = get_llm(state.api_key, state.model_name)
        final_summary = llm.invoke(final_summarize_prompt.format(summaries="\n\n".join(all_summaries)))
        final_summary = final_summary.content if hasattr(final_summary, "content") else str(final_summary)
        
        logging.info("‚úÖ Agent A3: ƒê√£ ph√¢n t√≠ch th√†nh c√¥ng")
        return {
            "summary": final_summary,
            "entities": final_entities,
            "retry_count_analyze": retry_count + 1,
            "error": None,
            "messages": state.get("messages", [])
        }
        
    except Exception as e:
        logging.error(f"‚ùå Agent A3: L·ªói khi ph√¢n t√≠ch - {str(e)}")
        return {
            "error": str(e),
            "retry_count_analyze": retry_count + 1,
            "messages": state.get("messages", [])
        }

verify_prompt = ChatPromptTemplate.from_messages([
    ("system", """X√°c minh t√≠nh ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß c·ªßa c√°c th·ª±c th·ªÉ (entities) ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t.
Nhi·ªám v·ª• c·ªßa b·∫°n:
1. Ki·ªÉm tra xem c√°c entities c√≥ li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ ch√≠nh kh√¥ng
2. X√°c minh t√≠nh ch√≠nh x√°c c·ªßa c√°c entities (t√™n, ng√†y th√°ng, ƒë·ªãa ƒëi·ªÉm, s·ªë li·ªáu)
3. ƒê√°nh gi√° m·ª©c ƒë·ªô ƒë·∫ßy ƒë·ªß c·ªßa th√¥ng tin ƒë√£ tr√≠ch xu·∫•t
4. Ph√°t hi·ªán c√°c th√¥ng tin quan tr·ªçng b·ªã b·ªè s√≥t

N·∫øu ph√°t hi·ªán v·∫•n ƒë·ªÅ, g·ª≠i message y√™u c·∫ßu x·ª≠ l√Ω l·∫°i (v√≠ d·ª•: {{"to": "agent_a1", "action": "retry_clean"}}).
Tr·∫£ v·ªÅ JSON: {{"verified": bool, "verified_data": dict, "message": dict}}"""),
    ("user", "Entities: {entities}")
])

def verified_agent(state: AgentState) -> AgentState:
    """Agent x√°c minh th√¥ng tin"""
    if not state.api_key or not state.model_name:
        return {"error": "API key and model name are required", "messages": state.get("messages", [])}
        
    logging.info("üöÄ Agent Verify: B·∫Øt ƒë·∫ßu x√°c minh k·∫øt qu·∫£...")
    if state["error"] or not state["entities"] or not state["db"]:
        logging.error(f"‚ùå Agent Verify: Thi·∫øu d·ªØ li·ªáu ƒë·ªÉ x√°c minh - {state['error'] or 'Missing data'}")
        return {"error": state["error"] or "Missing data", "messages": state.get("messages", [])}
    
    try:
        result = search_tool.invoke({
            "faiss_index": state["db"],
            "query": state["question"],
            "chunks": state["chunks"],
            "api_key": state.api_key,
            "embedding_model": state.embedding_model
        })
        
        llm = get_llm(state.api_key, state.model_name)
        response = llm.invoke(verify_prompt.format(
            question=state["question"],
            summary=state["summary"],
            entities=str(result["entities"])
        ))
        
        parsed = json.loads(response.content)
        messages = state.get("messages", []) + [parsed["message"]] if parsed["message"] else state.get("messages", [])
        
        if parsed["verified"]:
            logging.info("‚úÖ Agent Verify: X√°c minh th√†nh c√¥ng")
        else:
            logging.warning("‚ö†Ô∏è Agent Verify: X√°c minh th·∫•t b·∫°i, c·∫ßn ph√¢n t√≠ch l·∫°i")
            
        return {
            "verified_data": parsed["verified_data"],
            "messages": messages,
            "error": None if parsed["verified"] else "Verification failed"
        }
    except Exception as e:
        logging.error(f"‚ùå Agent Verify: L·ªói khi x√°c minh - {str(e)}")
        return {"error": str(e), "messages": state.get("messages", [])}


from pydantic import BaseModel, Field
class FinalOutput(BaseModel):
    answer: str = Field(description="C√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi")
    summary: str = Field(description="T√≥m t·∫Øt n·ªôi dung")
    entities: Dict[str, Any] = Field(description="Entities tr√≠ch xu·∫•t")
    verified_data: Dict[str, Any] = Field(description="D·ªØ li·ªáu ƒë√£ x√°c minh")

def aggregated_agent(state: AgentState) -> AgentState:
    """Agent t·ªïng h·ª£p k·∫øt qu·∫£ cu·ªëi c√πng"""
    if not state.api_key or not state.model_name:
        return {"error": "API key and model name are required", "messages": state.get("messages", [])}
        
    logging.info("üöÄ Agent Aggregate: B·∫Øt ƒë·∫ßu t·ªïng h·ª£p k·∫øt qu·∫£...")
    if state["error"]:
        logging.error(f"‚ùå Agent Aggregate: Kh√¥ng th·ªÉ t·ªïng h·ª£p do l·ªói - {state['error']}")
        return {"report": f"Error: {state['error']}", "messages": state.get("messages", [])}
    
    try:
        final_result = FinalOutput(
            answer=f"Response to '{state['question']}': {state['verified_data']}",
            summary=state["summary"],
            entities=state["entities"],
            verified_data=state["verified_data"]
        )
        
        logging.info("‚úÖ Agent Aggregate: ƒê√£ t·ªïng h·ª£p th√†nh c√¥ng")
        return {"report": final_result.model_dump_json(), "error": None, "messages": state.get("messages", [])}
    except Exception as e:
        logging.error(f"‚ùå Agent Aggregate: L·ªói khi t·ªïng h·ª£p - {str(e)}")
        return {
            "error": str(e),
            "messages": state.get("messages", []) + [{"to": "agent_verify", "action": "reverify"}]
        }


