from modules.tools import search_tool, extract_pdf, chunk_and_embed, check_chunks
from modules.states import AgentState
from langchain.prompts import ChatPromptTemplate
from typing import List, Dict, Any
import json
import logging
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from datetime import datetime

# Cáº¥u hÃ¬nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

load_dotenv()

def get_llm(api_key: str, model_name: str) -> ChatOpenAI:
    """Khá»Ÿi táº¡o LLM vá»›i API key vÃ  model Ä‘Æ°á»£c truyá»n vÃ o"""
    return ChatOpenAI(
        model=model_name,
        temperature=0,
        api_key=api_key
    )

def extracted_agent(state: AgentState) -> AgentState:
    logging.info("ğŸš€ Agent A1: Báº¯t Ä‘áº§u trÃ­ch xuáº¥t PDF...")
    retry_count = state.get("retry_count_a1", 0)
    if retry_count >= 3:
        logging.error("âŒ Agent A1: ÄÃ£ thá»­ 3 láº§n nhÆ°ng khÃ´ng thÃ nh cÃ´ng")
        return {"error": "Invalid PDF after 3 retries", "messages": state.get("messages", [])}
    
    messages = state.get("messages", [])
    if any(msg.get("to") == "agent_a1" and msg.get("action") == "retry_clean" for msg in messages):
        logging.info("ğŸ”„ Agent A1: Thá»­ láº¡i viá»‡c lÃ m sáº¡ch vÃ  OCR...")
    
    result = extract_pdf.invoke({"pdf_path": state["file_path"]})
    
    if result["error"]:
        logging.error(f"âŒ Agent A1: Lá»—i khi trÃ­ch xuáº¥t PDF - {result['error']}")
    else:
        logging.info(f"âœ… Agent A1: ÄÃ£ trÃ­ch xuáº¥t thÃ nh cÃ´ng {len(result['cleaned_text'])} kÃ½ tá»±")
    
    return {
        "cleaned_text": result["cleaned_text"],
        "error": result["error"],
        "retry_count_a1": retry_count + 1,
        "messages": messages
    }


def chunked_and_embedded_agent(state: AgentState) -> AgentState:
    """Agent phÃ¢n Ä‘oáº¡n vÃ  táº¡o embeddings"""
    logging.info("ğŸš€ Agent A2: Báº¯t Ä‘áº§u chia nhá» vÃ  táº¡o embeddings...")
    retry_count = state.get("retry_count_a2", 0)
    if retry_count >= 3:
        logging.error("âŒ Agent A2: ÄÃ£ thá»­ 3 láº§n nhÆ°ng khÃ´ng thÃ nh cÃ´ng")
        return {"error": "Invalid chunks after 3 retries", "messages": state.get("messages", [])}
    
    if state["error"] or not state["cleaned_text"]:
        logging.error(f"âŒ Agent A2: KhÃ´ng cÃ³ text Ä‘á»ƒ xá»­ lÃ½ - {state['error'] or 'No cleaned text'}")
        return {"error": state["error"] or "No cleaned text", "messages": state.get("messages", [])}
    
    messages = state.get("messages", [])
    chunk_size, chunk_overlap = 2000, 200
    for msg in messages:
        if msg.get("to") == "agent_a2" and msg.get("action") == "adjust_chunk":
            chunk_size = int(msg.get("chunk_size", 2000))
            chunk_overlap = int(msg.get("chunk_overlap", 200))
            logging.info(f"ğŸ”„ Agent A2: Äiá»u chá»‰nh kÃ­ch thÆ°á»›c chunk={chunk_size}, overlap={chunk_overlap}")
    
    # Táº¡o file_id tá»« file_path
    file_id = Path(state["file_path"]).stem if state.get("file_path") else "unknown"
    
    result = chunk_and_embed.invoke({
        "text": state["cleaned_text"],
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "file_id": file_id,
        "api_key": state.api_key,
        "embedding_model": state.embedding_model
    })

    if result.get("error"):
        logging.error(f"âŒ Agent A2: Lá»—i khi táº¡o embeddings - {result['error']}")
        return {
            "error": result["error"],
            "retry_count_a2": retry_count + 1,
            "messages": messages
        }

    if not check_chunks(result["chunks"]):
        logging.warning("âš ï¸ Agent A2: Chunks khÃ´ng há»£p lá»‡, thá»­ Ä‘iá»u chá»‰nh kÃ­ch thÆ°á»›c...")
        return {
            "error": "Invalid chunks",
            "retry_count_a2": retry_count + 1,
            "messages": messages + [{"to": "agent_a2", "action": "adjust_chunk", "chunk_size": chunk_size - 500, "chunk_overlap": chunk_overlap - 50}]
        }

    logging.info(f"âœ… Agent A2: ÄÃ£ táº¡o {len(result['chunks'])} chunks vÃ  embeddings thÃ nh cÃ´ng")
    return {
        "chunks": result["chunks"],
        "embeddings": result["embeddings"],
        "db": result["db"],
        "retry_count_a2": retry_count + 1,
        "error": None,
        "messages": messages
    }


summarize_prompt = ChatPromptTemplate.from_messages([
    ("system", """Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  trÃ­ch xuáº¥t thÃ´ng tin quan trá»ng, chÃ­nh xÃ¡c vÃ  ngáº¯n gá»n tá»« vÄƒn báº£n. 
Táº­p trung vÃ o:
1. Dá»¯ kiá»‡n chÃ­nh (facts) vÃ  thÃ´ng tin cá»‘t lÃµi
2. TÃªn ngÆ°á»i, tá»• chá»©c quan trá»ng
3. Äá»‹a Ä‘iá»ƒm vÃ  thá»i gian cá»¥ thá»ƒ
4. Sá»‘ liá»‡u Ä‘á»‹nh lÆ°á»£ng vÃ  thá»‘ng kÃª
5. Má»‘i quan há»‡ giá»¯a cÃ¡c thá»±c thá»ƒ

Bá» qua:
- ThÃ´ng tin trÃ¹ng láº·p
- Chi tiáº¿t khÃ´ng quan trá»ng
- Ná»™i dung mang tÃ­nh quáº£ng cÃ¡o
- ÄÃ¡nh giÃ¡ chá»§ quan

TÃ³m táº¯t ngáº¯n gá»n, sÃºc tÃ­ch (tá»‘i Ä‘a 100 tá»«), chá»‰ giá»¯ láº¡i thÃ´ng tin quan trá»ng nháº¥t."""),
    ("user", "{text}")
])

extract_prompt = ChatPromptTemplate.from_messages([
    ("system", """TrÃ­ch xuáº¥t cÃ¡c thá»±c thá»ƒ (entities) quan trá»ng tá»« vÄƒn báº£n sau má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§.
PhÃ¢n loáº¡i thÃ nh 4 nhÃ³m:
1. names: TÃªn ngÆ°á»i, tá»• chá»©c, cÃ´ng ty, thÆ°Æ¡ng hiá»‡u quan trá»ng
2. dates: NgÃ y thÃ¡ng, má»‘c thá»i gian, khoáº£ng thá»i gian
3. locations: Äá»‹a Ä‘iá»ƒm, quá»‘c gia, thÃ nh phá»‘, khu vá»±c Ä‘á»‹a lÃ½
4. numbers: Sá»‘ liá»‡u thá»‘ng kÃª, tiá»n tá»‡, pháº§n trÄƒm, sá»‘ Ä‘o lÆ°á»ng

Chá»‰ trÃ­ch xuáº¥t cÃ¡c entities thá»±c sá»± quan trá»ng vÃ  cÃ³ giÃ¡ trá»‹ thÃ´ng tin cao.
Bá» qua cÃ¡c entities khÃ´ng rÃµ rÃ ng hoáº·c khÃ´ng quan trá»ng.
Tráº£ vá» Ä‘Ãºng Ä‘á»‹nh dáº¡ng JSON: {{"entities": {{"names": [], "dates": [], "locations": [], "numbers": []}}}}"""),
    ("user", "{text}")
])

final_summarize_prompt = ChatPromptTemplate.from_messages([
    ("system", """Tá»•ng há»£p cÃ¡c pháº§n thÃ´ng tin Ä‘Ã£ Ä‘Æ°á»£c trÃ­ch xuáº¥t thÃ nh má»™t báº£n tÃ³m táº¯t hoÃ n chá»‰nh, ngáº¯n gá»n vÃ  cÃ³ cáº¥u trÃºc.
YÃªu cáº§u:
1. Ngáº¯n gá»n, sÃºc tÃ­ch, khÃ´ng quÃ¡ 500 tá»«
2. Chá»‰ giá»¯ láº¡i thÃ´ng tin quan trá»ng vÃ  cÃ³ giÃ¡ trá»‹ cao
3. Sáº¯p xáº¿p thÃ´ng tin theo thá»© tá»± logic vÃ  dá»… hiá»ƒu
4. LiÃªn káº¿t cÃ¡c thÃ´ng tin cÃ³ liÃªn quan vá»›i nhau
5. Äáº£m báº£o chÃ­nh xÃ¡c vÃ  khÃ¡ch quan
6. Táº­p trung vÃ o dá»¯ kiá»‡n, sá»‘ liá»‡u vÃ  má»‘i quan há»‡ giá»¯a cÃ¡c thá»±c thá»ƒ
7. Loáº¡i bá» thÃ´ng tin trÃ¹ng láº·p, khÃ´ng quan trá»ng hoáº·c mang tÃ­nh chá»§ quan

Má»¥c Ä‘Ã­ch: GiÃºp ngÆ°á»i Ä‘á»c náº¯m báº¯t nhanh chÃ³ng nhá»¯ng thÃ´ng tin quan trá»ng nháº¥t tá»« vÄƒn báº£n gá»‘c."""),
    ("user", "{summaries}")
])

def analyze_chunk_batch(chunk: str, api_key: str, model_name: str) -> Dict:
    """Xá»­ lÃ½ má»™t chunk Ä‘Æ¡n láº» - Ä‘á»ƒ dÃ¹ng trong parallel processing"""
    try:
        # Khá»Ÿi táº¡o LLM vá»›i API key vÃ  model Ä‘Æ°á»£c truyá»n vÃ o
        llm = get_llm(api_key, model_name)
        
        # Xá»­ lÃ½ summary
        summary_result = llm.invoke(summarize_prompt.format(text=chunk))
        summary = summary_result.content if hasattr(summary_result, "content") else str(summary_result)
        
        # Xá»­ lÃ½ entities vá»›i error handling tá»‘t hÆ¡n
        entities_result = llm.invoke(extract_prompt.format(text=chunk))
        
        # Fix: Xá»­ lÃ½ Ä‘Ãºng ChatPromptValue vÃ  cÃ¡c Ä‘á»‹nh dáº¡ng khÃ¡c
        if hasattr(entities_result, "content"):
            entities_content = entities_result.content
        elif hasattr(entities_result, "text"):
            entities_content = entities_result.text
        else:
            entities_content = str(entities_result)
        
        # Parse JSON entities
        try:
            if isinstance(entities_content, str):
                entities_data = json.loads(entities_content)
                if "entities" in entities_data:
                    entities = entities_data["entities"]
                else:
                    entities = entities_data
            else:
                entities = {"names": [], "dates": [], "locations": [], "numbers": []}
        except (json.JSONDecodeError, KeyError) as e:
            logging.warning(f"âš ï¸ KhÃ´ng thá»ƒ parse entities JSON: {str(e)}")
            entities = {"names": [], "dates": [], "locations": [], "numbers": []}
        
        # Äáº£m báº£o entities cÃ³ Ä‘Ãºng structure
        if not isinstance(entities, dict):
            entities = {"names": [], "dates": [], "locations": [], "numbers": []}
        
        for key in ["names", "dates", "locations", "numbers"]:
            if key not in entities:
                entities[key] = []
            elif not isinstance(entities[key], list):
                entities[key] = []
        
        result = {
            "summary": summary,
            "entities": entities
        }
        
        logging.info(f"âœ… Chunk processed: {len(chunk)} chars, entities: {sum(len(v) for v in entities.values())} items")
        return result
        
    except Exception as e:
        logging.error(f"âŒ Error processing chunk: {str(e)}")
        return {
            "summary": "Error processing this chunk",
            "entities": {"names": [], "dates": [], "locations": [], "numbers": []}
        }

def analyze_batch_parallel(batch: List[str], api_key: str, model_name: str, max_workers: int = 10) -> List[Dict]:
    """Xá»­ lÃ½ song song cÃ¡c chunks"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(analyze_chunk_batch, chunk, api_key, model_name) for chunk in batch]
        results = []
        for future in as_completed(futures):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                logging.error(f"âŒ Lá»—i khi xá»­ lÃ½ chunk: {str(e)}")
                results.append({"summary": "", "entities": {"names": [], "dates": [], "locations": [], "numbers": []}})
    return results

def count_tokens_estimate(text: str) -> int:
    """Æ¯á»›c tÃ­nh sá»‘ tokens (khoáº£ng 3.5 kÃ½ tá»± = 1 token cho tiáº¿ng Viá»‡t)"""
    return len(text) // 3

def chunk_summaries_for_final(summaries: List[str], max_tokens: int = 150000) -> List[List[str]]:
    """Chia summaries thÃ nh cÃ¡c chunks lá»›n hÆ¡n Ä‘á»ƒ táº­n dá»¥ng token limit cao"""
    chunks = []
    current_chunk = []
    current_tokens = 0
    
    for summary in summaries:
        summary_tokens = count_tokens_estimate(summary)
        
        if current_tokens + summary_tokens > max_tokens and current_chunk:
            chunks.append(current_chunk)
            current_chunk = [summary]
            current_tokens = summary_tokens
        else:
            current_chunk.append(summary)
            current_tokens += summary_tokens
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks

def analyzed_agent(state: AgentState) -> AgentState:
    """Agent phÃ¢n tÃ­ch ná»™i dung"""
    if not state.api_key or not state.model_name:
        return {"error": "API key and model name are required", "messages": state.get("messages", [])}
        
    logging.info("ğŸš€ Agent A3: Báº¯t Ä‘áº§u phÃ¢n tÃ­ch ná»™i dung...")
    retry_count = state.get("retry_count_analyze", 0)
    if retry_count >= 3:
        logging.error("âŒ Agent A3: ÄÃ£ thá»­ 3 láº§n nhÆ°ng khÃ´ng thÃ nh cÃ´ng")
        return {"error": "Analysis failed after 3 retries", "messages": state.get("messages", [])}
    
    if state["error"] or not state["chunks"]:
        logging.error(f"âŒ Agent A3: KhÃ´ng cÃ³ chunks Ä‘á»ƒ phÃ¢n tÃ­ch - {state['error'] or 'No chunks'}")
        return {"error": state["error"] or "No chunks", "messages": state.get("messages", [])}
    
    try:
        # Xá»­ lÃ½ song song cÃ¡c chunks
        results = analyze_batch_parallel(state["chunks"], state.api_key, state.model_name)
        
        # Tá»•ng há»£p káº¿t quáº£
        all_summaries = []
        all_entities = {"names": set(), "dates": set(), "locations": set(), "numbers": set()}
        
        for result in results:
            if "summary" in result:
                all_summaries.append(result["summary"])
            
            if "entities" in result:
                entities = result["entities"]
                for key in all_entities:
                    if key in entities:
                        all_entities[key].update(entities[key])
        
        # Chuyá»ƒn set thÃ nh list
        final_entities = {k: sorted(list(v)) for k, v in all_entities.items()}
        
        # Tá»•ng há»£p summary cuá»‘i cÃ¹ng
        llm = get_llm(state.api_key, state.model_name)
        final_summary = llm.invoke(final_summarize_prompt.format(summaries="\n\n".join(all_summaries)))
        final_summary = final_summary.content if hasattr(final_summary, "content") else str(final_summary)
        
        logging.info("âœ… Agent A3: ÄÃ£ phÃ¢n tÃ­ch thÃ nh cÃ´ng")
        return {
            "summary": final_summary,
            "entities": final_entities,
            "retry_count_analyze": retry_count + 1,
            "error": None,
            "messages": state.get("messages", [])
        }
        
    except Exception as e:
        logging.error(f"âŒ Agent A3: Lá»—i khi phÃ¢n tÃ­ch - {str(e)}")
        return {
            "error": str(e),
            "retry_count_analyze": retry_count + 1,
            "messages": state.get("messages", [])
        }

verify_prompt = ChatPromptTemplate.from_messages([
    ("system", """XÃ¡c minh tÃ­nh chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§ cá»§a cÃ¡c thá»±c thá»ƒ (entities) Ä‘Ã£ Ä‘Æ°á»£c trÃ­ch xuáº¥t.
Nhiá»‡m vá»¥ cá»§a báº¡n:
1. Kiá»ƒm tra xem cÃ¡c entities cÃ³ liÃªn quan Ä‘áº¿n chá»§ Ä‘á» chÃ­nh khÃ´ng
2. XÃ¡c minh tÃ­nh chÃ­nh xÃ¡c cá»§a cÃ¡c entities (tÃªn, ngÃ y thÃ¡ng, Ä‘á»‹a Ä‘iá»ƒm, sá»‘ liá»‡u)
3. ÄÃ¡nh giÃ¡ má»©c Ä‘á»™ Ä‘áº§y Ä‘á»§ cá»§a thÃ´ng tin Ä‘Ã£ trÃ­ch xuáº¥t
4. PhÃ¡t hiá»‡n cÃ¡c thÃ´ng tin quan trá»ng bá»‹ bá» sÃ³t

Náº¿u phÃ¡t hiá»‡n váº¥n Ä‘á», gá»­i message yÃªu cáº§u xá»­ lÃ½ láº¡i (vÃ­ dá»¥: {{"to": "agent_a1", "action": "retry_clean"}}).
Tráº£ vá» JSON: {{"verified": bool, "verified_data": dict, "message": dict}}"""),
    ("user", "Entities: {entities}")
])

def verified_agent(state: AgentState) -> AgentState:
    """Agent xÃ¡c minh thÃ´ng tin"""
    if not state.api_key or not state.model_name:
        return {"error": "API key and model name are required", "messages": state.get("messages", [])}
        
    logging.info("ğŸš€ Agent Verify: Báº¯t Ä‘áº§u xÃ¡c minh káº¿t quáº£...")
    if state["error"] or not state["entities"] or not state["db"]:
        logging.error(f"âŒ Agent Verify: Thiáº¿u dá»¯ liá»‡u Ä‘á»ƒ xÃ¡c minh - {state['error'] or 'Missing data'}")
        return {"error": state["error"] or "Missing data", "messages": state.get("messages", [])}
    
    try:
        result = search_tool.invoke({
            "faiss_index": state["db"],
            "query": state["question"],
            "chunks": state["chunks"],
            "api_key": state.api_key,
            "embedding_model": state.embedding_model
        })
        
        llm = get_llm(state.api_key, state.model_name)
        response = llm.invoke(verify_prompt.format(
            question=state["question"],
            summary=state["summary"],
            entities=str(result["entities"])
        ))
        
        parsed = json.loads(response.content)
        messages = state.get("messages", []) + [parsed["message"]] if parsed["message"] else state.get("messages", [])
        
        if parsed["verified"]:
            logging.info("âœ… Agent Verify: XÃ¡c minh thÃ nh cÃ´ng")
        else:
            logging.warning("âš ï¸ Agent Verify: XÃ¡c minh tháº¥t báº¡i, cáº§n phÃ¢n tÃ­ch láº¡i")
            
        return {
            "verified_data": parsed["verified_data"],
            "messages": messages,
            "error": None if parsed["verified"] else "Verification failed"
        }
    except Exception as e:
        logging.error(f"âŒ Agent Verify: Lá»—i khi xÃ¡c minh - {str(e)}")
        return {"error": str(e), "messages": state.get("messages", [])}


from pydantic import BaseModel, Field
class FinalOutput(BaseModel):
    answer: str = Field(description="CÃ¢u tráº£ lá»i cho cÃ¢u há»i")
    summary: str = Field(description="TÃ³m táº¯t ná»™i dung")
    entities: Dict[str, Any] = Field(description="Entities trÃ­ch xuáº¥t")
    verified_data: Dict[str, Any] = Field(description="Dá»¯ liá»‡u Ä‘Ã£ xÃ¡c minh")

def aggregated_agent(state: AgentState) -> AgentState:
    """Agent tá»•ng há»£p káº¿t quáº£ cuá»‘i cÃ¹ng"""
    if not state.api_key or not state.model_name:
        return {"error": "API key and model name are required", "messages": state.get("messages", [])}
        
    logging.info("ğŸš€ Agent Aggregate: Báº¯t Ä‘áº§u tá»•ng há»£p káº¿t quáº£...")
    if state["error"]:
        logging.error(f"âŒ Agent Aggregate: KhÃ´ng thá»ƒ tá»•ng há»£p do lá»—i - {state['error']}")
        return {"report": f"Error: {state['error']}", "messages": state.get("messages", [])}
    
    try:
        final_result = FinalOutput(
            answer=f"Response to '{state['question']}': {state['verified_data']}",
            summary=state["summary"],
            entities=state["entities"],
            verified_data=state["verified_data"]
        )
        
        logging.info("âœ… Agent Aggregate: ÄÃ£ tá»•ng há»£p thÃ nh cÃ´ng")
        return {"report": final_result.model_dump_json(), "error": None, "messages": state.get("messages", [])}
    except Exception as e:
        logging.error(f"âŒ Agent Aggregate: Lá»—i khi tá»•ng há»£p - {str(e)}")
        return {
            "error": str(e),
            "messages": state.get("messages", []) + [{"to": "agent_verify", "action": "reverify"}]
        }


